### SDSC5001

## Exploration

### Types of Data and Variables

Data Types

-   **Data Matrix**: Often structured as an $n \times p$ matrix, where $n$ is the number of objects and $p$ represents variables. Each row corresponds to an object, and each column to a variable.
-   **Other Data Forms**: Text, image, audio, video, transaction, and graph data.

Variable Types

-   **Continuous Variables** (e.g., height, time)
-   **Nominal Variables** åä¹‰å˜é‡(e.g., gender, eye color)
-   **Ordinal Variables** åºæ•°å˜é‡(e.g., satisfaction levels like â€œdislike,â€ â€œneutral,â€ â€œlikeâ€)
    å’Œ **Nominal Variables** ä¸åŒï¼Œæœ‰é¡ºåº
-   **Interval Variables** (e.g., temperature)

### Data Quality

-   **Quality Issues**: Include noise, outliers, missing values, and sampling bias.
    æ•°æ®å™ªå£°NoiseæŒ‡çš„æ˜¯åŸå§‹å€¼çš„æ‰°åŠ¨ï¼Œè€Œå¼‚å¸¸å€¼outliersåˆ™æ˜¯ä¸å…¶ä»–è§‚å¯Ÿå€¼ç›¸æ¯”æ˜¾è‘—ä¸åŒçš„è§‚å¯Ÿå€¼
-   **Handling Missing Values**: Strategies such as imputation or using partial information.

## Overview

### General Model and Error

If we have some samples, we assume the data are generated from
$$
Y=f(x)+\epsilon
$$

-   $f$ is some unknown function 
    -   Parametric models å‚æ•°ä¼°è®¡
        -   Linear/polynomial regression model
        -   Deep learning
    -   Nonparametric models æ— å‚æ•°ä¼°è®¡
        -   **classification and regression tree**
        -   SVM
        -   Smoothing
-   $\epsilon$ is a random error with a mean of 0 and independent of X

>   Why $\epsilon$ needs mean equal 0?
>
>   1.   ä¿è¯æ— åä¼°è®¡
>   2.   ç®€åŒ–åˆ†æ

Prediciton can be represented by: $\hat{Y}=\hat{f}(X)$

### Prediction and Inference

-   **Inference**: Understanding **how** and **why** variables relate to each other.
    æˆ‘ä»¬æ›´åŠ å…³ç³»å†…åœ¨çš„é€»è¾‘å…³ç³»

-   **Prediction**: Using data to **forecast** future outcomes.
    æˆ‘ä»¬åªå…³å¿ƒæ˜¯å¦èƒ½é¢„æµ‹åˆ°æ­£ç¡®çš„ç»“æ„ï¼Œä¸å…³å¿ƒRelationshipæ˜¯æ€ä¹ˆæ ·çš„

For example, deep learning is actually more **focused on prediction than inference**. We only care about whether we can predict the right outputs.

### Model Assessment for Regression

MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰è¢«å®šä¹‰ä¸ºå¯¹ \(X\) å’Œ \(Y\) çš„æœŸæœ›ï¼š

$$
MSE(f) = E[(Y - f(X))^2]
$$
 $\frac{1}{n} \sum_{i=1}^{n} (y_i - f(x_i))^2$æ˜¯åŸºäºæ ·æœ¬å‡æ–¹è¯¯å·®Training Errorï¼Œæ˜¯**ä¼°è®¡é‡**

The MSE can be written as the sum of the [variance](https://en.wikipedia.org/wiki/Variance) of the estimator and the squared [bias](https://en.wikipedia.org/wiki/Bias_of_an_estimator) of the estimator, providing a useful way to calculate the MSE and implying that in the case of unbiased estimators, **the MSE and variance are equivalent**.
MSE å¯ä»¥å†™æˆä¼°è®¡å€¼çš„æ–¹å·®å’Œä¼°è®¡å€¼çš„å¹³æ–¹åå·®ä¹‹å’Œï¼Œæä¾›äº†ä¸€ä¸ªè®¡ç®— MSE çš„æœ‰ç”¨çš„æ–¹æ³•ï¼Œå¹¶è¡¨æ˜åœ¨æ— åä¼°è®¡å€¼çš„æƒ…å†µä¸‹ï¼Œ**MSE å’Œæ–¹å·®æ˜¯ç­‰ä»·çš„**, è§Bias-Variance Decomposition
$$
\text{MSE}(\hat{\theta}) = \text{Var}_{\theta}(\hat{\theta}) + \text{Bias}(\hat{\theta}, \theta)^2
$$

### Bias-Variance Decomposition

åœ¨å¾ˆå¤šæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å‡è®¾è¾“å‡º \(Y\) ç”±ä»¥ä¸‹å…³ç³»ç”Ÿæˆï¼š

$$
Y = f(X) + \epsilon
$$
åœ¨åå·®-æ–¹å·®åˆ†è§£ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ†ææ¨¡å‹é¢„æµ‹å€¼ $\hat{f}(X)$ å’ŒçœŸå®å€¼ \(Y\) ä¹‹é—´çš„è¯¯å·®ï¼š

$$
E[(Y - \hat{f}(X))^2]
$$
åˆ©ç”¨ $Y = f(X) + \epsilon$ è¿™ä¸€äº‹å®ï¼Œæˆ‘ä»¬å°†è¯¯å·®åˆ†è§£ä¸ºï¼š

$$
E[(Y - \hat{f}(X))^2] = E[(f(X) + \epsilon - \hat{f}(X))^2]
$$
å±•å¼€è¿™ä¸ªå¹³æ–¹é¡¹ï¼š

$$
E[(f(X) - \hat{f}(X))^2] + E[\epsilon^2] + 2E[(f(X) - \hat{f}(X))\epsilon]
$$
å› ä¸º $\epsilon$ å’Œ $X$ æ˜¯ç‹¬ç«‹çš„ï¼Œ$E[\epsilon] = 0$ï¼Œæœ€åä¸€é¡¹æ¶ˆå¤±ï¼Œäºæ˜¯æˆ‘ä»¬å¾—åˆ°ï¼š

$$
E[(Y - \hat{f}(X))^2] = E[(f(X) - \hat{f}(X))^2] + \text{var}(\epsilon)
$$
ç”±äºMSEå¯ä»¥å†™æˆä¼°è®¡å€¼çš„æ–¹å·®å’Œä¼°è®¡å€¼çš„å¹³æ–¹åå·®ä¹‹å’Œï¼Œå…¶å°†è¯¯å·®åˆ†è§£ï¼š
$$
E[(Y - \hat{f}(X))^2] = \text{Bias}(\hat{f}(X)) + \text{var}(\hat{f}(X)) + \text{var}(\epsilon)
$$
**åå·®ä¸æ–¹å·®çš„è§£é‡Š**

-   **åå·®** $\text{Bias}(\hat{f}(X))$ ï¼šè¡¨ç¤ºç”±äº**è¿‘ä¼¼çœŸå®å‡½æ•° f å¼•å…¥çš„è¯¯å·®**ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ç”¨ç®€å•çš„çº¿æ€§æ¨¡å‹å»æ‹Ÿåˆéçº¿æ€§çš„æ•°æ®ï¼Œåå·®ä¼šå¾ˆå¤§ã€‚

-   **æ–¹å·®** $\text{var}(\hat{f}(X))$ ï¼šåæ˜ æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å˜åŒ–ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤æ‚çš„æ¨¡å‹ï¼Œå°½ç®¡å®ƒåœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†**åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå¯èƒ½ä¼šè¡¨ç°å¾—éå¸¸ä¸åŒ**ï¼Œå³æ–¹å·®ä¼šå¾ˆå¤§ã€‚

-   **å™ªå£°é¡¹** $\text{var}(\epsilon)$ï¼šè¿™æ˜¯ä¸å¯é¿å…çš„å™ªå£°ï¼Œå¯¹æ‰€æœ‰æ¨¡å‹æ¥è¯´éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¸èƒ½é€šè¿‡æ”¹è¿›æ¨¡å‹æ¥å‡å°ã€‚

### **æ¨¡å‹å¤æ‚åº¦å¯¹åå·®å’Œæ–¹å·®çš„å½±å“**

ä¸€èˆ¬æ¥è¯´ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨æ›´**å¤æ‚çš„æ¨¡å‹**æ—¶ï¼š

-   **åå·®ä¼šå‡å°**ï¼Œå› ä¸ºæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
-   **æ–¹å·®ä¼šå¢åŠ **ï¼Œå› ä¸ºå¤æ‚æ¨¡å‹æ›´å®¹æ˜“å—åˆ°è®­ç»ƒæ•°æ®ä¸­å™ªå£°çš„å½±å“ï¼Œä»è€Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚

æœ€ç»ˆï¼Œæˆ‘ä»¬éœ€è¦åœ¨åå·®å’Œæ–¹å·®ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œä»¥æœ€å°åŒ–æ€»ä½“è¯¯å·®ã€‚

### Test Error in Practice

1. **æŒ‘æˆ˜**ï¼š

    -   **æµ‹è¯•é›†ä¸å¯ç”¨**ï¼šåœ¨å®é™…ä¸­ï¼Œå¯èƒ½ç¼ºä¹è¶³å¤Ÿçš„æ•°æ®æ¥åˆ’åˆ†å‡ºç‹¬ç«‹çš„æµ‹è¯•é›†ã€‚

2. **è§£å†³æ–¹æ³•**ï¼š

    -   è°ƒæ•´è®­ç»ƒè¯¯å·®ä¼°è®¡æµ‹è¯•è¯¯å·®
        -   **AICï¼ˆAkaike Information Criterion èµ¤æ± ä¿¡æ¯å‡†åˆ™ï¼‰**ï¼šè€ƒè™‘æ¨¡å‹æ‹Ÿåˆåº¦å’Œå¤æ‚åº¦ã€‚
        -   **BICï¼ˆBayesian Information Criterion è´å¶æ–¯ä¿¡æ¯å‡†åˆ™ï¼‰**ï¼šå¯¹æ¨¡å‹å¤æ‚åº¦æ–½åŠ æ›´å¼ºçš„æƒ©ç½šã€‚
        -   **åæ–¹å·®æƒ©ç½š**ï¼ˆCovariance Penaltyï¼‰ï¼šé€šè¿‡æƒ©ç½šé¡¹è°ƒæ•´æ¨¡å‹çš„å¤æ‚åº¦ã€‚

    -   **é€šè¿‡è®­ç»ƒé›†åˆ’åˆ†ä¼°è®¡æµ‹è¯•è¯¯å·® Validation Set Approach**

![image-2024101125443471â€¯AM](./SDSC5001.assets/image-2024101125443471.png)

### Validation Set Approach

å°†æ•°æ®é›†ç®€å•åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

**ä¼˜ç‚¹ (Advantages):**

1.  **Simple idea**
2.  **æ˜“äºå®ç° (Easy to implement):**

**ç¼ºç‚¹ (Disadvantages):**

1.  **éªŒè¯é›†çš„å‡æ–¹è¯¯å·® (MSE) å¯èƒ½é«˜åº¦ä¸ç¨³å®š (Validation MSE can be highly variable):** éªŒè¯é›†çš„ MSE å–å†³äºæ•°æ®é›†çš„åˆ’åˆ†ï¼Œä¸åŒçš„åˆ’åˆ†å¯èƒ½å¯¼è‡´è¯¯å·®æ³¢åŠ¨è¾ƒå¤§ï¼Œå°¤å…¶åœ¨æ ·æœ¬é‡è¾ƒå°æ—¶ã€‚
2.  **ä»…ä½¿ç”¨éƒ¨åˆ†æ•°æ®æ‹Ÿåˆæ¨¡å‹ (Only a subset of observations are used to fit the model):** åªæœ‰ä¸€éƒ¨åˆ†æ•°æ®ç”¨äºè®­ç»ƒï¼Œå¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆï¼Œå› ä¸ºæ¨¡å‹æœªèƒ½åˆ©ç”¨å…¨éƒ¨æ•°æ®æ¥å‘ç°æ½œåœ¨æ¨¡å¼ã€‚

>   Cross-validation is used to estimate how a model generalizes to an unseendataset.

### LOOCV

ç•™ä¸€æ³•äº¤å‰éªŒè¯ (Leave-One-Out Cross Validation, LOOCV) åŸºæœ¬æ€æƒ³ï¼šå¯¹äºå¤§å°ä¸º $n$ çš„æ•°æ®é›†ï¼Œæ¯æ¬¡ç•™å‡ºä¸€ä¸ªæ•°æ®ç‚¹ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™ $n - 1$ ä¸ªæ•°æ®ç‚¹ç”¨äºè®­ç»ƒã€‚

**LOOCV çš„æ­¥éª¤**

-   **åˆ’åˆ†æ•°æ®é›†:**

    -   **è®­ç»ƒé›†**ï¼šå¤§å°ä¸º$n - 1$

    -   **éªŒè¯é›†**ï¼šå¤§å°ä¸º 1

-   **è®­ç»ƒæ¨¡å‹å¹¶è®¡ç®— MSE**

-   **é‡å¤ $n$ æ¬¡ï¼š** æ¯æ¬¡ç•™å‡ºä¸åŒçš„æ•°æ®ç‚¹

-   **è®¡ç®—å¹³å‡ MSEï¼š** æ‰€æœ‰ $n$ æ¬¡å®éªŒçš„ MSE å¹³å‡å€¼ä½œä¸ºæ¨¡å‹çš„æœ€ç»ˆè¯¯å·®ä¼°è®¡

![ ](./SDSC5001.assets/image-2024101130414435.png)

**ä¼˜ç‚¹:**

-   **ä½¿ç”¨å‡ ä¹å…¨éƒ¨çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼š**é¿å…äº†å› æ•°æ®åˆ’åˆ†å¯¼è‡´çš„è¯¯å·®æ³¢åŠ¨
-   **æ›´å¥½åœ°ä¼°è®¡æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ï¼š**æ¯æ¬¡ä»…ç•™å‡ºä¸€ä¸ªæ•°æ®ç‚¹ç”¨äºéªŒè¯

**ç¼ºç‚¹:**

-   **è®¡ç®—æˆæœ¬é«˜ (Computationally intensive)ï¼š**éœ€è¦è®­ç»ƒ $n$ æ¬¡æ¨¡å‹ï¼Œè®¡ç®—é‡å¤§ã€‚
-   **æ–¹å·®è¾ƒå¤§ï¼š**éªŒè¯é›†éå¸¸å°ï¼Œå¯èƒ½å¯¼è‡´**é«˜æ–¹å·®**ï¼Œå°¤å…¶åœ¨æ•°æ®æœ‰å™ªå£°çš„æƒ…å†µä¸‹ã€‚

#### LOOCV vs. Validation Set Approach

**ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼ˆLOOCVï¼‰**å’Œ**éªŒè¯é›†æ–¹æ³•ï¼ˆValidation Set Approachï¼‰**çš„ä¼˜ç¼ºç‚¹

1. **LOOCV çš„ä¼˜ç‚¹**ï¼š

- **åå·®è¾ƒå°ï¼ˆLess biasï¼‰**ï¼š
  - ç”±äº LOOCV çš„è®­ç»ƒé›†åŒ…å«äº† \( n - 1 \) ä¸ªæ•°æ®ç‚¹ï¼ˆå‡ ä¹æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼‰ï¼Œå› æ­¤è®­ç»ƒé›†éå¸¸æ¥è¿‘æ•´ä¸ªæ•°æ®é›†ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å……åˆ†åœ°å­¦ä¹ æ•°æ®ï¼Œå› æ­¤å®ƒçš„åå·®æ¯”éªŒè¯é›†æ–¹æ³•æ›´å°ã€‚

- **äº§ç”Ÿæ›´å°‘çš„å‡æ–¹è¯¯å·®æ³¢åŠ¨ï¼ˆLess variable MSEï¼‰**ï¼š
  - LOOCV è¿›è¡Œ \( n \) æ¬¡éªŒè¯ï¼Œæ¯æ¬¡ç•™ä¸€ä¸ªæ•°æ®ç‚¹åšéªŒè¯ï¼Œè¿™æ„å‘³ç€å®ƒèƒ½å¤Ÿå¾ˆå¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå‡å°‘éªŒè¯è¯¯å·®çš„æ³¢åŠ¨æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒéªŒè¯é›†æ–¹æ³•ä½¿ç”¨å•ä¸€åˆ’åˆ†è¿›è¡ŒéªŒè¯ï¼Œå› æ­¤éªŒè¯è¯¯å·®æ³¢åŠ¨æ›´å¤§ã€‚

2. **LOOCV çš„ç¼ºç‚¹**ï¼š

- **è®¡ç®—é‡å¤§ï¼ˆComputationally intensiveï¼‰**: LOOCV çš„ä¸»è¦ç¼ºç‚¹æ˜¯è®¡ç®—é‡éå¸¸å¤§ã€‚å› ä¸ºå®ƒéœ€è¦å¯¹æ¯ä¸ªæ•°æ®ç‚¹éƒ½è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œé‡å¤ \( n \) æ¬¡ï¼Œå› æ­¤å½“æ•°æ®é›†è¾ƒå¤§æ—¶ï¼ŒLOOCV ä¼šæ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºã€‚è¿™æ˜¯å®ƒçš„ä¸€ä¸ªä¸»è¦åŠ£åŠ¿ï¼Œå°¤å…¶åœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶æ˜¾å¾—ä¸å¤ªå®ç”¨ã€‚

### K-Fold CV

**æ¦‚è¿°**ï¼šå°†æ•°æ®é›†åˆ†æˆ $K$ ä¸ªæŠ˜ï¼Œæ¯æ¬¡ç”¨å…¶ä¸­ $K-1$ ä¸ªæŠ˜è®­ç»ƒï¼Œå‰©ä¸‹çš„ä¸€ä¸ªæŠ˜éªŒè¯ï¼Œé‡å¤ $K$ æ¬¡

**ä¼˜åŠ¿**ï¼šç›¸æ¯” LOOCVï¼ŒK æŠ˜äº¤å‰éªŒè¯åœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œæ˜¯å®é™…åº”ç”¨ä¸­å¸¸ç”¨çš„æ–¹æ³•ã€‚

![image-2024101130749113â€¯AM](./SDSC5001.assets/image-2024101130749113.png)

#### LOOCV vs. K-Fold CV

- **LOOCV åå·®å°ï¼ŒK-Fold CVåå·®å¤§**ï¼šLOOCV æ¯æ¬¡ä½¿ç”¨ \( n-1 \) ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå‡ ä¹æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼Œå› æ­¤å®ƒçš„**åå·®è¾ƒå°**ï¼Œåä¹‹K-Fold CVåå·®è¾ƒå¤§
- **LOOCV æ–¹å·®å¤§ï¼ŒK-Fold CVåå·®å¤§**ï¼šLOOCVæ¯æ¬¡åªç”¨ä¸€ä¸ªæ•°æ®ç‚¹ä½œä¸ºéªŒè¯é›†ï¼Œæ¨¡å‹çš„è¡¨ç°å¯èƒ½åœ¨ä¸åŒçš„æ•°æ®ç‚¹ä¸Šæœ‰è¾ƒå¤§æ³¢åŠ¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒK-Fold CV ä½¿ç”¨è¾ƒå¤§çš„éªŒè¯é›†ï¼Œå› æ­¤æ–¹å·®è¾ƒå°ã€‚
- åœ¨é€‰æ‹©ä½¿ç”¨ LOOCV è¿˜æ˜¯ K-Fold CV æ—¶ï¼Œéœ€è¦åœ¨**åå·®**å’Œ**æ–¹å·®**ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚
- ç»éªŒè¡¨æ˜ï¼Œ5 æŠ˜æˆ– 10 æŠ˜äº¤å‰éªŒè¯å¯ä»¥æä¾›åˆç†çš„æµ‹è¯•è¯¯å·®ä¼°è®¡ï¼Œä¸”è®¡ç®—æ•ˆç‡ç›¸å¯¹è¾ƒé«˜ï¼Œæˆä¸ºå®è·µä¸­çš„å¸¸ç”¨æ–¹æ³•ã€‚

## Linear Regression

### Simple Linear Regression

Linear regression model assumes that
$$
f(x) = \beta_0 + \beta_1 x
$$
**Minimize the least square error**
$$
(\hat{\beta}_0, \hat{\beta}_1) = \arg\min_{\beta_0, \beta_1} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2
$$

åˆ©ç”¨æ±‚å¯¼è§£ï¼Œ**Solution is**

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \quad \text{are the fitted values.}
$$

$$
e_i = y_i - \hat{y}_i \quad \text{are the residuals.}
$$
### Multiple Linear Regression

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \epsilon
$$

### Assessing the Accuracy of the Model

#### Total sum of squares (TSS)

$$
TSS=\sum_{i=1}^n(\hat{y}-\bar{y})^2
$$

#### Residual sum of squares (RSS)

$$
\text{RSS} = e_1^2 + e_2^2 + \cdots + e_n^2,
$$

or, in Simple Linear Regression, equivalently as

$$
\text{RSS} = (y_1 - \hat{\beta}_0 - \hat{\beta}_1 x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1 x_2)^2 + \cdots + (y_n - \hat{\beta}_0 - \hat{\beta}_1 x_n)^2.
$$
#### Residual standard error (RSE)

residual standard error æ˜¯ç”¨äºè¡¡é‡çº¿æ€§å›å½’æ¨¡å‹ä¸­è¯¯å·®é¡¹ï¼ˆæˆ–æ®‹å·®ï¼‰çš„æ ‡å‡†å·®ã€‚å®ƒè¡¨ç¤ºæ¨¡å‹é¢„æµ‹å€¼ä¸å®é™…è§‚æµ‹å€¼ä¹‹é—´çš„å¹³å‡åå·®ç¨‹åº¦ï¼Œä¹Ÿå³æ¨¡å‹æœªèƒ½è§£é‡Šçš„æ•°æ®æ³¢åŠ¨æƒ…å†µ
- RSE è¶Šå°ï¼Œæ„å‘³ç€æ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆè¶Šå¥½ï¼Œé¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„åå·®è¶Šå°
- åœ¨å®é™…åˆ†æä¸­ï¼Œæ¯ä½“è¯¯å·®é¡¹çš„æ–¹å·®$\sigma^2$å¾€å¾€æœªçŸ¥ï¼ŒRSE å¯ä»¥ä½œä¸º$\sigma$ çš„ä¸€ä¸ªä¼°è®¡å€¼ï¼Œä»è€Œç”¨äºè®¡ç®—å›å½’ç³»æ•°çš„æ ‡å‡†è¯¯å·®ï¼ˆSEï¼‰ã€‚ä¾‹å¦‚ï¼Œ$\text{SE}(\hat{\beta}_1) å’Œ \text{SE}(\hat{\beta}_0)$ æ˜¯åœ¨ç”¨ RSE ä½œä¸º $\sigma$ çš„ä¼°è®¡å€¼çš„åŸºç¡€ä¸Šè®¡ç®—çš„

$$
RSE=Â \text{SE}(\epsilon)=\hat{\sigma}=\sqrt{RSS/(n-p-1)}
$$

#### $R^2$ for Regression

The **$R^2$ Statistic** is
  $$
  R^2 = \frac{TSS-RSS}{RSS} = 1 - \frac{RSS}{TSS}
  $$
- it always takes on a value between 0 and 1
- independent of the scale of $Y$
  ä¸å•ä½æ— å…³

In Simple Linear Regression:  $R^2 = [cor(X,Y)]^2$
In Multiple Linear Regression: $R^2 = [cor(Y,\hat{Y})]^2$

> In Simple Linear Regression $R^2 = r^2$
> In multiple regression,Â  $R^2$Â  is used to assess the **overall explanatory power** of the model with multiple predictors, whileÂ  $r^2$Â  typically applies to the correlation between **two individual variables**.

**Interpretation of**Â  $R^2$ï¼šrepresents the proportion of variabilityå˜å¼‚æ€§ inÂ  $Y$Â  that can be explained by the predictor variableÂ  $X$ . è¡¨ç¤ºå›å½’æ¨¡å‹èƒ½å¤Ÿè§£é‡Šçš„å› å˜é‡å˜åŒ–çš„æ¯”ä¾‹

> What is the meaning of 1âˆ’R2?
> $1 - R^2$æ˜¯æœªè¢«æ¨¡å‹è§£é‡Šçš„å› å˜é‡å˜åŒ–çš„æ¯”ä¾‹ï¼Œåæ˜ äº†æ¨¡å‹çš„ä¸è¶³ä¹‹å¤„

#### The problem of $R^2$

R2 will always **increase** when **more variables are added to the model**, even if those variables are only weakly associated with the response.
å½“æ¨¡å‹ä¸­åŠ å…¥æ›´å¤šå˜é‡æ—¶ï¼ŒR2 æ€»æ˜¯ä¼šå¢åŠ ï¼Œå³ä½¿è¿™äº›å˜é‡ä¸å“åº”çš„ç›¸å…³æ€§å¾ˆå¼±

æ¯å¢åŠ ä¸€ä¸ªå˜é‡ï¼Œå›å½’æ¨¡å‹è·å¾—äº†ä¸€ä¸ªé¢å¤–çš„å‚æ•°æ¥æ›´å¥½åœ°æ‹Ÿåˆæ ·æœ¬æ•°æ®ã€‚è¿™äº›é¢å¤–çš„å‚æ•°å¯ä»¥è¿›ä¸€æ­¥å‡å°‘æ®‹å·®å¹³æ–¹å’ŒÂ  $\text{RSS}$ ï¼Œå³ä½¿è¿™ä¸ªå˜é‡å¯¹è§£é‡Šå“åº”å˜é‡å‡ ä¹æ²¡æœ‰è´¡çŒ®ã€‚è¿™ç§è¿‡æ‹Ÿåˆçš„è¶‹åŠ¿ä½¿å¾—æ¨¡å‹è¶Šæ¥è¶Šç²¾ç»†åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä»è€Œä½¿Â  $R^2$Â  å¢å¤§ã€‚

> è¿›ä¸€æ­¥ç†è§£ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ— å…³å˜é‡, **æ‰¾å‡ºä½¿å¾—å…¨å±€RSSæ›´å°çš„å‚æ•°ç»„åˆ**ï¼Œä½¿å¾—æ¨¡å‹â€œè¿‡æ‹Ÿåˆâ€æˆ–â€œé”™è¯¯æ‹Ÿåˆâ€ï¼Œå®é™…ä¸Šç”±äºæ— å…³å˜é‡çš„å½±å“ï¼Œå¯¼è‡´çœŸæ­£å†³å®šæ€§çš„å˜é‡çš„æ‹Ÿåˆç¨‹åº¦åè€Œé™ä½äº†ï¼Œè™½ç„¶$R^2$ æé«˜äº†ä½†æ˜¯æ€§èƒ½å´æ˜¯é™ä½çš„

è§£å†³ï¼š
$$
\text{Adjusted } R^2 = 1 - \frac{\text{RSS}/(n - p - 1)}{\text{TSS}/(n - 1)}
$$
It can be shown that in this simple linear regression setting that  where $r$ is the correlation between $X$ and $Y$, in Simple Linear Regression.

### Point Estimation

åœ¨[ç»Ÿè®¡å­¦](https://zh.wikipedia.org/wiki/ç»Ÿè®¡å­¦)ä¸­ï¼Œç‚¹ä¼°è®¡ï¼ˆpoint estimationï¼‰æ˜¯æŒ‡ä»¥[æ ·æœ¬](https://zh.wikipedia.org/wiki/æ ·æœ¬)æ•°æ®æ¥ä¼°è®¡[æ€»ä½“](https://zh.wikipedia.org/wiki/æ€»ä½“)[å‚æ•°](https://zh.wikipedia.org/wiki/æ¯æ•¸)ï¼Œ ä¼°è®¡ç»“æœä½¿ç”¨ä¸€ä¸ªç‚¹çš„æ•°å€¼è¡¨ç¤ºâ€œæœ€ä½³ä¼°è®¡å€¼â€ï¼Œå› æ­¤ç§°ä¸ºç‚¹ä¼°è®¡ã€‚ç”±æ ·æœ¬æ•°æ®ä¼°è®¡æ€»ä½“åˆ†å¸ƒæ‰€å«æœªçŸ¥å‚æ•°çš„çœŸå®å€¼ï¼Œæ‰€å¾—åˆ°çš„å€¼ï¼Œç§°ä¸ºä¼°è®¡å€¼ã€‚

#### standard error of $\hat\mu$

æˆ‘ä»¬å¯ä»¥ç”¨æ ·æœ¬ä¼°ç®— $\hat{\mu}$ ï¼Œbut how accurate is the sample mean $\hat{\mu}$ as an estimate of $\mu$? We answer this by computing **standard error** of $\mu$:
$$
\text{Var}(\hat{\mu}) = \text{SE}(\hat{\mu})^2 = \frac{\sigma^2}{n},
$$
- $\sigma$ is the RSE

#### Point Estimation of $\beta_0$ and $\beta_1$

Assume that
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$
where $\epsilon_i$'s are i.i.d. from $N(0, \sigma^2)$.

To compute the standard errors  associated with $Î²^0$ and $Î²^1$, we use the following formulas:
$$
\text{SE}(\hat{\beta}_0)^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right], \quad \text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2}.
$$
where $\sigma^2=Var(\epsilon)=RSE^2$

when $\bar{x}=0$ ï¼Œ $SE(\hat{\mu_1})=SE(\hat\mu)$

It can be shown that

$$
\hat{\beta}_1 \sim N\left(\beta_1, \frac{\sigma^2}{\sum_i (x_i - \bar{x})^2}\right)
$$

$$
\hat{\beta}_0 \sim N\left(\beta_0, \left(\frac{1}{n} + \frac{\bar{x}^2}{\sum_i (x_i - \bar{x})^2}\right) \sigma^2\right)
$$


#### Confidence Interval

Standard errors can be used to compute confidence intervals. 

å¯¹äºä»»æ„æœä»æ­£æ€åˆ†å¸ƒçš„ä¼°è®¡é‡ï¼ˆå‡å€¼ä¸º $\hat{\mu}$ï¼Œæ ‡å‡†è¯¯å·®ä¸º $\text{SE}$ï¼‰ï¼Œåœ¨95%çš„ç½®ä¿¡æ°´å¹³ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å†™å‡ºå…¶ç½®ä¿¡åŒºé—´ä¸ºï¼š
$$
\hat{\mu} \pm 1.96 \cdot \text{SE}
$$

åŒç†ï¼Œ
$$
\left[ \hat{\beta}_1 - 2 \cdot \text{SE}(\hat{\beta}_1), \; \hat{\beta}_1 + 2 \cdot \text{SE}(\hat{\beta}_1) \right]
$$
ç½®ä¿¡åŒºé—´å¯ä»¥ç”¨äºåˆ¤æ–­å‚æ•°æ˜¯å¦æ˜¾è‘—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ $\beta_1$ çš„ç½®ä¿¡åŒºé—´ä¸º [0.042, 0.053]ï¼Œä¸”ä¸åŒ…å«0ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰95%çš„ä¿¡å¿ƒè®¤ä¸ºå¹¿å‘Šæ”¯å‡ºå¯¹é”€å”®é‡æœ‰æ˜¾è‘—çš„æ­£å‘å½±å“

#### Alternative hypothesis

#### At least one of the X useful in predicting

æˆ‘ä»¬æƒ³è¯æ˜æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªå˜é‡ $X_n$ æ˜¯å¯¹ç»“æœæœ‰å½±å“çš„ï¼Œå…¶ç­‰ä»·ä¸ç”¨å‡è®¾æ£€éªŒè¯æ˜
$$
y_i = \beta_0 + \beta_1 x_i + \beta_px_p+...+ \epsilon_i
$$
- **Null Hypothesis** ($H_0$): All regression coefficients are zero ($\beta_1 = \beta_2 = \dots = \beta_p = 0$), implying no relationship between any predictor and the response.
- **Alternative Hypothesis** ($H_a$): At least one $\beta_j$ is non-zero, suggesting a relationship between at least one predictor and the response.

This hypothesis test is performed by computing the **F-Statistic**:

$$
F = \frac{(TSS - RSS) / p}{RSS / (n - p - 1)}
$$
- If $H_0$ is true (no relationship exists), the F-statistic should be close to 1.
- If $H_a$ is true (at least one predictor has an effect), F is expected to be greater than 1.


### Potential Problems
#### Multicollinearity (Collinearity)

-   ç†æƒ³æƒ…å†µï¼šåœ¨å¤šå…ƒå›å½’åˆ†æä¸­ï¼Œç†æƒ³çŠ¶æ€ä¸‹ï¼Œæ‰€æœ‰è‡ªå˜é‡ï¼ˆé¢„æµ‹å˜é‡ï¼‰ä¹‹é—´åº”è¯¥æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå³ä¸å­˜åœ¨çº¿æ€§å…³ç³»æˆ–é«˜åº¦ç›¸å…³æ€§ã€‚
-   å¤šé‡å…±çº¿æ€§multicollinearityï¼ˆå…±çº¿æ€§ï¼‰: è‡ªå˜é‡ä¹‹é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§

**ç¤ºä¾‹**ï¼š

![](./SDSC5001.assets/file-20241113193901062.png)
**Newspaper** advertising does not have a direct effect on **sales** (p>0.0001,å‡è®¾æ£€éªŒåŒæ„åŸå‡è®¾$H_0$,å³ç³»æ•°ä¸º0,æ— å…³), but due to its correlation with **radio** (which does affect sales), it appears to be associated with **sales** in simpler analyses. This demonstrates the importance of considering multicollinearity and the relationships among predictors in multiple regression analysis.

**é—®é¢˜**ï¼š

-   å¤šé‡å…±çº¿æ€§ä¼šå¯¼è‡´ä¸¥é‡çš„é—®é¢˜ï¼Œä¾‹å¦‚å¯¼è‡´å›å½’ç³»æ•°ä¼°è®¡çš„ä¸ç¨³å®šã€æ ‡å‡†è¯¯çš„å¢å¤§ï¼Œä»è€Œå½±å“å›å½’æ¨¡å‹çš„è§£é‡Šæ€§å’Œé¢„æµ‹èƒ½åŠ›ã€‚
-   åœ¨å‡ºç°å¤šé‡å…±çº¿æ€§çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½éš¾ä»¥ç¡®å®šå“ªäº›è‡ªå˜é‡å¯¹å› å˜é‡æœ‰æ˜¾è‘—å½±å“ï¼Œå› ä¸ºå®ƒä»¬çš„å½±å“å¯èƒ½äº’ç›¸æŠµæ¶ˆã€‚

è§£å†³ï¼š

- ä½¿ç”¨VIPæ£€æµ‹å¤šé‡å…±çº¿æ€§é—®é¢˜
- ä½¿ç”¨PCAå¯ä»¥å‡å°‘æ•°æ®é›†çš„ç»´æ•°
- ä½¿ç”¨ç‰¹å¾é€‰æ‹©å‰”é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾

#### VIF

**æ–¹å·®è†¨èƒ€å› å­ï¼ˆVariance Inflation Factor, VIFï¼‰**ï¼Œç”¨äºæ£€æµ‹å¤šé‡å…±çº¿æ€§é—®é¢˜
$$
(VIF)_j=\frac{1}{1-R^2_j}
$$
å…¶ä¸­ï¼Œ$R_j^2$ æ˜¯å½“ç¬¬ $j$ ä¸ªè‡ªå˜é‡å¯¹å…¶ä»– $p-1$ ä¸ªè‡ªå˜é‡è¿›è¡Œå›å½’æ—¶å¾—åˆ°çš„å†³å®šç³»æ•°ã€‚$R_j^2$ è¶Šé«˜ï¼Œè¡¨ç¤ºç¬¬ $j$ ä¸ªè‡ªå˜é‡å’Œå…¶ä»–è‡ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§è¶Šå¼ºã€‚ä¸€èˆ¬æ¥è®²ï¼Œå¦‚æœ[æ–¹å·®è†¨èƒ€å› å­](https://baike.baidu.com/item/%E6%96%B9%E5%B7%AE%E8%86%A8%E8%83%80%E5%9B%A0%E5%AD%90/4993652?fromModule=lemma_inlink)è¶…è¿‡10ï¼Œåˆ™å›å½’æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§ã€‚

#### Non-linearity of the Data

![](./SDSC5001.assets/file-20241115012950726.png)

**Residual plots** are a useful graphical tool for identifying non-linearity.

If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the  predictors, such as $\log X$, $\sqrt{X}$ and $X^2$, in the regression model.

### Correlation of Error Terms

è¯¯å·®é¡¹æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚å¦‚æœè¯¯å·®é¡¹å½¼æ­¤ç›¸å…³ï¼Œå…¶æ ·æœ¬æ ‡å‡†è¯¯å·®/æ–¹å·®çš„ä¼°è®¡å€¼å¾€å¾€ä¼šè¢«ä½ä¼°ï¼Œè¿›è€Œå¯¼è‡´ç½®ä¿¡åŒºé—´å’Œé¢„æµ‹åŒºé—´è¿‡äºç‹­çª„ï¼Œpå€¼ä¹Ÿå¯èƒ½ä¼šè¢«ä½ä¼°ï¼Œä½¿å¾—æˆ‘ä»¬æ›´å®¹æ˜“è¯¯åˆ¤æŸä¸ªå‚æ•°å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œä»è€Œå¯¹æ¨¡å‹çš„å¯ä¿¡åº¦äº§ç”Ÿä¸åˆç†çš„ä¿¡å¿ƒã€‚

ä¾‹å¦‚ï¼Œå¦‚æœå¸‚åœºåœ¨ä»Šå¤©é­å—äº†è´Ÿé¢æ¶ˆæ¯çš„å½±å“è€Œå‡ºç°ä¸‹è·Œï¼Œæ˜å¤©å¯èƒ½ä»ç„¶ä¼šå—æ­¤å½±å“ï¼Œé€ æˆé¢„æµ‹è¯¯å·®åå·®ä¸€è‡´ã€‚å› æ­¤ï¼Œä»Šå¤©çš„è¯¯å·®é¡¹å’Œæ˜å¤©çš„è¯¯å·®é¡¹å¯èƒ½æ˜¯æ­£ç›¸å…³çš„ã€‚

### Non-constancy of Error Variance (Heteroscedasticity)

è¯¯å·®æ–¹å·®ä¸æ’å®šï¼ˆå¼‚æ–¹å·®æ€§ï¼‰: åœ¨å›å½’åˆ†æä¸­ï¼Œå¦‚æœè¯¯å·®çš„æ–¹å·®éšé¢„æµ‹å€¼å˜åŒ–è€Œå˜åŒ–ï¼Œå°±ç§°ä¸º**å¼‚æ–¹å·®æ€§**ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„æ®‹å·®ï¼ˆ$e_i$ï¼‰çš„æ–¹å·®åº”è¯¥æ˜¯æ’å®šçš„ï¼ˆåŒæ–¹å·®æ€§ï¼‰ï¼Œå³åœ¨æ‰€æœ‰æ‹Ÿåˆå€¼ $\hat{y}_i$ ä¸Šï¼Œæ®‹å·®çš„åˆ†å¸ƒåº”å¤§è‡´ç›¸åŒã€‚

å¦‚æœè¯¯å·®æ–¹å·®ä¸æ’å®šï¼ˆå­˜åœ¨å¼‚æ–¹å·®æ€§ï¼‰ï¼Œæ¨¡å‹çš„é¢„æµ‹å¯èƒ½ä¼šæœ‰åå·®ï¼Œå½±å“å›å½’ç³»æ•°çš„æ˜¾è‘—æ€§æ£€éªŒ

æ®‹å·® $e_i$ ä¸æ‹Ÿåˆå€¼ $\hat{y}_i$ çš„æ•£ç‚¹å›¾: 

![image-20241026121935595â€¯PM](./SDSC5001.assets/image-20241026121935595.png)

-   åœ¨å·¦ä¾§å›¾ä¸­ï¼Œæ®‹å·®éšæ‹Ÿåˆå€¼çš„å¢å¤§è€Œå‘ˆç°â€œæ¼æ–—å½¢â€åˆ†å¸ƒï¼Œå³è¯¯å·®æ–¹å·®åœ¨æ‹Ÿåˆå€¼è¾ƒå¤§æ—¶å¢åŠ ã€‚è¿™ç§æ¨¡å¼è¡¨æ˜è¯¯å·®æ–¹å·®ä¸æ’å®šï¼Œå­˜åœ¨å¼‚æ–¹å·®æ€§
-   å³ä¾§å›¾é€šè¿‡å¯¹ $Y$ **å–å¯¹æ•°å˜æ¢**åï¼Œæ®‹å·®åˆ†å¸ƒæ›´åŠ å‡åŒ€ï¼Œå‡å°‘äº†æ–¹å·®è†¨èƒ€çš„é—®é¢˜ï¼Œè¿™å¯èƒ½æ˜¯å¯¹å¼‚æ–¹å·®æ€§çš„ä¸€ç§æœ‰æ•ˆè°ƒæ•´


### hierarchical principle

The hierarchical principle states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.
åˆ†å±‚åŸåˆ™æŒ‡å‡ºï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ¨¡å‹ä¸­åŒ…å«äº¤äº’ä½œç”¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¹Ÿåº”åŒ…å«ä¸»æ•ˆåº”ï¼Œå³ä½¿ä¸ä¸»æ•ˆåº”ç³»æ•° ç›¸å…³çš„ p å€¼å¹¶ä¸æ˜¾è‘—ã€‚

## Classification

### General Setup

Assume $y \in \{1, \dots, K\}$ is a qualitative response variable and $\mathbf{x} \in \mathbb{R}^p$ represents a feature vector.

A **classifier** $G: \mathbb{R}^p \rightarrow \{1, \dots, K\}$ is designed to assign a feature vector $\mathbf{x}$ to one of the $K$ classes.

The objective of a good classifier $G(\mathbf{x})$ is to minimize the misclassification error, defined as:
$$
\text{err}(G) = P(y \neq G(\mathbf{x})) = \mathbb{E}\left[I(y \neq G(\mathbf{x}))\right]
$$
where $I(\cdot)$ is an **indicator function** that is 1 if $y \neq G(\mathbf{x})$ and 0 otherwise.

#### Concepts

**Classification Function**: Define the classification function for each class as $h_k(\mathbf{x}): \mathbb{R}^p \rightarrow \mathbb{R}$, where $k = 1, \dots, K$.
- è¿™ä¸ªå‡½æ•°è¡¨ç¤ºç±»åˆ«Â $k$ çš„åˆ†ç±»å‡½æ•°ã€‚å¯¹äºæ¯ä¸€ä¸ªç±»åˆ«Â $k$ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªç‹¬ç«‹çš„åˆ†ç±»å‡½æ•°Â $h_k$ï¼Œå…¶å€¼ç”¨äºè¡¡é‡ç‰¹å¾å‘é‡ $\mathbf{x}$ å±äºè¯¥ç±»åˆ«çš„â€œå€¾å‘â€æˆ–â€œå¾—åˆ†â€
- è¾“å…¥ç©ºé—´Â $ $\mathbb{R}^p$ ï¼š$\mathbf{x}$ æ˜¯ä¸€ä¸ªÂ  $p$Â  ç»´çš„ç‰¹å¾å‘é‡ï¼Œè¡¨ç¤ºä¸º $\mathbb{R}^p$ã€‚è¿™æ„å‘³ç€ç‰¹å¾ç©ºé—´æœ‰Â  $p$Â  ä¸ªç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œå¯èƒ½åŒ…å«ä¸åŒçš„å˜é‡æˆ–å±æ€§ï¼‰
- è¾“å‡ºç©ºé—´Â  $\mathbb{R}$ ï¼š $h_k(\mathbf{x})$Â  çš„è¾“å‡ºæ˜¯ä¸€ä¸ªå®æ•°ã€‚è¿™ä¸ªè¾“å‡ºå€¼å¯ä»¥ç”¨æ¥è¡¡é‡è¾“å…¥$\mathbf{x}$ å±äºç±»åˆ«Â $k$Â çš„å¯èƒ½æ€§æˆ–ç›¸å…³åº¦
- Â $k = 1, \dots, K$ ï¼šè¿™æ˜¯å¯¹æ‰€æœ‰å¯èƒ½çš„ç±»åˆ«Â $k$ çš„ç´¢å¼•ï¼Œè¡¨ç¤ºåˆ†ç±»ä»»åŠ¡ä¸­å…±æœ‰Â $K$ ä¸ªç±»åˆ«

**Classifier**: $G(\mathbf{x}) = \arg\max_k \, h_k(\mathbf{x})$

**Estimated Classifier**: Based on the available **training data**, we estimate $h_k(\mathbf{x})$ and get the estimated classifier $\hat{G}(\mathbf{x})$:
$$
\hat{G}(\mathbf{x}) = \arg\max_k \, \hat{h}_k(\mathbf{x})
$$

**Classification Boundary**: The boundary between classes $k$ and $l$ is defined by:
$$
\{\mathbf{x} : h_k(\mathbf{x}) = h_l(\mathbf{x})\}
$$

### Linear Regression for Classification

**Bad way** to model Linear Regression:

Suppose $Y$ has three levels: type 1, type 2 and gestational

$$
Y = \begin{cases}Â 
1 & \text{if type 1} \\
2 & \text{if type 2} \\
3 & \text{if gestational}Â 
\end{cases}
$$
![](./SDSC5001.assets/file-20241108193737985.png)
Why bad?
- ä½¿ç”¨Â $Y = 1, 2, 3$ å¯¹ç±»åˆ«è¿›è¡Œç¼–ç ï¼Œä¼šéšå«ç±»åˆ«ä¹‹é—´å­˜åœ¨é¡ºåºå…³ç³»ï¼Œå³ç±»å‹ 1 < ç±»å‹ 2 < å¦Šå¨ ç³–å°¿ç—…ç±»å‹ã€‚å®é™…ä¸Šï¼Œ**è¿™äº›ç±»åˆ«ä¹‹é—´æ²¡æœ‰è¿™ç§å¤§å°æˆ–é¡ºåºå…³ç³»**ï¼Œå®ƒä»¬åªæ˜¯ä¸åŒçš„åˆ†ç±»æ ‡ç­¾ã€‚
- è‹¥ç›´æ¥ä½¿ç”¨æ­¤ç¼–ç è¿›è¡Œçº¿æ€§å›å½’æ¨¡å‹è®­ç»ƒï¼Œ**æ¨¡å‹ä¼šå°è¯•æ‰¾åˆ°ç±»åˆ«é—´çš„çº¿æ€§å…³ç³»**ï¼Œ**ä¾‹å¦‚è®¤ä¸ºç±»å‹ 2 çš„ç‰¹å¾åº”ä½äºç±»å‹ 1 å’Œå¦Šå¨ ç³–å°¿ç—…ä¹‹é—´**ã€‚è¿™ä¸åˆ†ç±»çš„å®é™…è¦æ±‚ä¸ç¬¦ï¼Œå› ä¸ºåˆ†ç±»ä»»åŠ¡å¹¶ä¸å…³å¿ƒç±»åˆ«çš„æ•°å€¼é¡ºåºï¼Œåªå…³å¿ƒç±»åˆ«çš„åŒºåˆ†ã€‚
- è¿™ç§ç¼–ç æ–¹å¼éšå«äº†ç±»åˆ«ä¹‹é—´çš„â€œ**è·ç¦»**â€ï¼Œä¾‹å¦‚ç±»åˆ« 1 å’Œç±»åˆ« 2 ä¹‹é—´çš„è·ç¦»æ˜¯ 1ï¼Œè€Œç±»åˆ« 2 å’Œç±»åˆ« 3 ä¹‹é—´çš„è·ç¦»ä¹Ÿæ˜¯ 1ã€‚ç„¶è€Œï¼Œåœ¨å®é™…ä¸­ï¼Œç±»åˆ«ä¹‹é—´æ²¡æœ‰å®šä¹‰è¿™ç§è·ç¦»ï¼Œç±»å‹ 1 å’Œå¦Šå¨ ç³–å°¿ç—…ç±»å‹ä¹‹é—´çš„â€œå·®å¼‚â€ä¸ç­‰äºå…¶ä»–ç±»åˆ«ä¹‹é—´çš„å·®å¼‚ã€‚

**Better way:** **ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰**
$$
y = \begin{cases}Â 
  3 \\Â 
  2 \\Â 
  1Â 
\end{cases}Â 
\rightarrowÂ 
\mathbf{Y} = \begin{pmatrix}Â 
  0 & 0 & 1 \\Â 
  0 & 1 & 0 \\Â 
  1 & 0 & 0Â 
\end{pmatrix}
$$

Further Issues:
- ä½¿ç”¨çº¿æ€§å›å½’ä¼°è®¡å¾—åˆ°çš„åˆ†ç±»å‡½æ•°Â  $\hat{h}_k(\mathbf{x})$Â  å¯èƒ½ä¼šå‡ºç°ä¸åˆç†çš„æƒ…å†µï¼Œå¦‚è¾“å‡ºå€¼å°äº 0 æˆ–å¤§äº 1ï¼Œå¯¼è‡´æ¦‚ç‡ä¼°è®¡å¤±çœŸã€‚è¿™ç§æƒ…å†µä¼šé™ä½æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§
- æ©è”½é—®é¢˜ï¼ˆMasking Problemï¼‰ï¼šåœ¨ä¸€äº›å¤šç±»åˆ«åˆ†ç±»é—®é¢˜ä¸­ï¼Œå¯èƒ½å‡ºç°ä¸€ä¸ªç±»åˆ«è¢«å…¶ä»–ç±»åˆ«â€œæ©ç›–â€æˆ–â€œéšè—â€çš„æƒ…å†µã€‚å…·ä½“è€Œè¨€ï¼Œå½“å¤šä¸ªç±»åˆ«çš„æ¦‚ç‡éå¸¸æ¥è¿‘æ—¶ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•å¾ˆå¥½åœ°åŒºåˆ†å®ƒä»¬ï¼Œå¯¼è‡´ç±»åˆ«ä¹‹é—´çš„æ··æ·†

æ€»çš„æ¥è¯´ï¼šçº¿æ€§å›å½’å¯ä»¥ç”¨ä½œåˆ†ç±»é—®é¢˜ï¼Œä½†æ˜¯åœ¨ç¼–ç æ—¶éœ€è¦ä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼Œå¹¶ä¸”å¯èƒ½å­˜åœ¨è¾“å‡ºå¤±çœŸå’Œæ©ç›–é—®é¢˜

### Bayes Rule

è´å¶æ–¯æ³•åˆ™ï¼ˆBayes Ruleï¼‰æ˜¯æ¦‚ç‡è®ºä¸­çš„ä¸€ä¸ªåŸºæœ¬å®šç†ï¼Œç”¨äºè®¡ç®—åœ¨ç»™å®šæ¡ä»¶ä¸‹äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚

è´å¶æ–¯æ³•åˆ™çš„æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š
$$
p_k(x)=P(Y = k | \mathbf{x}) = \frac{\pi_k f_k(x) }{\sum_{l=1}^X \pi_lf_l(x)}
$$

å…¶ä¸­ï¼š

â€¢ $p_k(X)=P(Y = k | \mathbf{x})$Â  æ˜¯åéªŒæ¦‚ç‡
â€¢ $f_k(X)=P(\mathbf{x} | Y = k)$Â  æ˜¯ä¼¼ç„¶æ¦‚ç‡
â€¢ $\pi_k=P(Y = k)$Â  æ˜¯å…ˆéªŒæ¦‚ç‡
â€¢ $\sum_{l=1}^X \pi_lf_l(x)$ æ˜¯è¾¹é™…æ¦‚ç‡

> å¯¹äºåˆ†ç±»é—®é¢˜å¯ä»¥ç®€å•æ¯”è¾ƒ$p_k(x)=\pi_k f_k(x)$å³å¯

The optimal classifier is:

$$
G^*(X) = \arg\max_k \, p_k(X)
$$
- Some methods attempt to estimate $p_k(X)$
  - Discriminant analysis, logistic regression, classification tree, deep neural network
- Other methods attempt to estimate $G^*(X)$ directly
  - Support vector machine, Boosting, Bagging

### Linear Discriminant Analysis (LDA)

LDAåŸºäºä»¥ä¸‹å‡è®¾ï¼šÂ $X | y = k \sim \mathcal{N}_p(\mu_k, \Sigma)$
- æ¯ä¸ªç±»åˆ«çš„æ•°æ®åœ¨ç‰¹å¾ç©ºé—´ä¸­æœä»**å¤šå…ƒæ­£æ€åˆ†å¸ƒ**
- **æ‰€æœ‰ç±»åˆ«å…±äº«ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ $\Sigma$ï¼Œå³å„ç±»åˆ«æ•°æ®çš„åˆ†å¸ƒå½¢çŠ¶ç›¸åŒï¼Œä½†å‡å€¼å¯ä»¥ä¸åŒ**
- æ¯ä¸ªç±»åˆ« k çš„å…ˆéªŒæ¦‚ç‡è®°ä¸º$\pi_k = P(Y = k)$ï¼Œæ˜¯ç±»åˆ« k çš„æ•´ä½“æ¯”ä¾‹

#### Discriminant functions åˆ¤åˆ«å‡½æ•°

To classify at the value $X = x$, we need to see which of the $p_k(x)$ is **largest**. Taking logs, and discarding terms that do not depend on $k$, we see that this is equivalent to assigning $x$ to the class with the largest *discriminant score*:

$$
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$$

Note that $\delta_k(x)$ is a *linear* function of $x$.


å…¶ä¸­åéªŒæ¦‚ç‡åˆ©ç”¨å¯¹æ•°æ¯”è¾ƒï¼š
$$
\log \frac{p_k(X)}{p_l(X)} > 0 \iff p_k(X) > p_l(X)
$$
å½“å¯¹æ•°æ¯”å¤§äºé›¶æ—¶ï¼Œè¡¨ç¤ºÂ  $p_k(X) > p_l(X)$ ï¼Œå³åœ¨ç»™å®šç‰¹å¾Â  $X$Â  çš„æƒ…å†µä¸‹ï¼Œæ›´å€¾å‘äºç±»åˆ«Â  $k$ 

åˆ©ç”¨LDAå±•å¼€æ¯”è¾ƒå‡½æ•°ï¼š
$$
\log \frac{p_k(X)}{p_l(X)} = \log \left( \frac{\pi_k}{\pi_l} \right) - \frac{1}{2} (\mu_k + \mu_l)^T \Sigma^{-1} (\mu_k - \mu_l) + X^T \Sigma^{-1} (\mu_k - \mu_l)
$$

æˆ‘ä»¬å‘ç°ï¼Œåœ¨å±•å¼€çš„æ—¶å€™ï¼Œç”±äºå…±äº« $\Sigma$ ,å¯¼è‡´ä¸€ä¸ª $X$ çš„äºŒæ¬¡é¡¹è¢«å»é™¤ï¼Œæ‰€ä»¥è¿™ä¸ªæ–¹ç¨‹æ˜¯ä¸€ä¸ªXçš„çº¿æ€§å‡½æ•°ï¼Œå› æ­¤LDAçš„å†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„ï¼ˆå³ä¸€ä¸ªè¶…å¹³é¢ï¼‰ï¼Œå¯ä»¥é€šè¿‡æ‰¾åˆ°è¯¥çº¿æ€§å‡½æ•°çš„æœ€å¤§å€¼æ¥ç¡®å®šåˆ†ç±»

> Englishï¼š The quadratic terms of ğ‘‹ vanish because of the equal covariance assumption across classes.

#### Gaussian density
The Gaussian density has the form
$$
f_k(x) = \frac{1}{\sqrt{2 \pi} \sigma_k} e^{-\frac{1}{2} \left( \frac{x - \mu_k}{\sigma_k} \right)^2}

$$
Plugging this into Bayes' formula, we get a rather complex expression for $p_k(x) = \Pr(Y = k | X = x)$:
$$
p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \left( \frac{x - \mu_k}{\sigma} \right)^2}}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{1}{2} \left( \frac{x - \mu_l}{\sigma} \right)^2}}
$$

#### Decision Boundary for K = 2 in LDA

**LDA boundary:**

Recall the definition of classification boundary:
$$
\{\mathbf{x} : h_k(\mathbf{x}) = h_l(\mathbf{x})\}
$$
Substitute the discriminant score into it:
$$
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$$
$$
\Rightarrow \delta_1(x) = \delta_2(x)
$$
$$
\Rightarrow x=\frac{\mu_1+\mu_2}{2}
$$

![](./SDSC5001.assets/file-20241108202859689.png)

- On the left: the probability density functions (pdf) of the two class distributions.
- On the right: the sample histogram, with the **solid black line** representing the fitted **LDA decision boundary**, and the dashed line indicating the **Bayes rule boundary**.

Bayes rule boundary æ€ä¹ˆæ±‚çš„ï¼Ÿ

Bayes rule is defined as

$$
G^*(X) = \arg\max_k P(Y = k \mid X)
$$
To the left of the intersection point x= 0, the probability $P(Y = 0|X) >P(Y = 1|X)$ as the corresponding pdf curve is above the other. Similarly, to the right of this point, $P(Y = 0|X) >P(Y = 1|X)$.

æ¢è¨€ä¹‹ï¼Œä¸¤ä¸ªå‡½æ•°çš„pdfçš„äº¤ç‚¹å°±æ˜¯æ»¡è¶³è´å¶æ–¯è§„åˆ™çš„åˆ†å‰²ç‚¹

#### Parameter estimation

Estimate $\mu_k$ by centroidè´¨å¿ƒ in class ğ‘˜ :
$$
\hat{\mu}_k = \frac{1}{n_k} \sum{\{i: y_i = k\}} X_i
$$
Estimate $\Sigma$ by pooledåˆå¹¶ within-class covariance matrix:
$$
\hat{\Sigma} = \frac{1}{n - K} \sum_{i=1}^n (X_i - \hat{\mu}_{y_i})(X_i - \hat{\mu}_{y_i})^T
$$

#### Parameter estimation Example:

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„åˆ†ç±»ä»»åŠ¡ï¼Œæœ‰ä¸¤ç±»æ•°æ®ï¼Œç‰¹å¾ç©ºé—´æ˜¯äºŒç»´çš„,è®­ç»ƒæ•°æ®å¦‚ä¸‹ï¼š

| ç±»åˆ« y | $X_1$ | $X_2$ |
| ---- | ----- | ----- |
| 1    | 5.1   | 3.5   |
| 1    | 4.9   | 3.0   |
| 1    | 5.0   | 3.2   |
| 2    | 6.1   | 2.9   |
| 2    | 6.3   | 3.3   |
| 2    | 6.5   | 3.0   |

æˆ‘ä»¬éœ€è¦ç”¨è¿™äº›æ•°æ®æ¥ä¼°è®¡LDAçš„å‚æ•°ã€‚

1. ä¼°è®¡ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ $\pi_k$
$$
  \hat{\pi}_1 = \frac{3}{6} = 0.5
$$
$$
  \hat{\pi}_2 = \frac{3}{6} = 0.5
$$
2. ä¼°è®¡å‡å€¼å‘é‡ $\mu_k$

ç±»åˆ« $k$çš„å‡å€¼å‘é‡ $\mu_k$ æ˜¯è¯¥ç±»åˆ«ä¸­æ‰€æœ‰æ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„å¹³å‡å€¼ã€‚
$$
  \hat{\mu}_1 = \begin{pmatrix} \frac{5.1 + 4.9 + 5.0}{3} \\ \frac{3.5 + 3.0 + 3.2}{3} \end{pmatrix} = \begin{pmatrix} 5.0 \\ 3.23 \end{pmatrix}
$$
$$
  \hat{\mu}_2 = \begin{pmatrix} \frac{6.1 + 6.3 + 6.5}{3} \\ \frac{2.9 + 3.3 + 3.0}{3} \end{pmatrix} = \begin{pmatrix} 6.3 \\ 3.07 \end{pmatrix}
$$

3. ä¼°è®¡åæ–¹å·®çŸ©é˜µ $\Sigma$

ç”±äºLDAå‡è®¾æ‰€æœ‰ç±»åˆ«å…±äº«ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ $\Sigma$ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—â€œåˆå¹¶çš„ç±»å†…åæ–¹å·®çŸ©é˜µâ€ï¼Œè¿™æ˜¯åœ¨æ‰€æœ‰æ ·æœ¬ä¸­æ±‚åæ–¹å·®ã€‚å…¬å¼ä¸ºï¼š
$$
\hat{\Sigma} = \frac{1}{n - K} \sum_{i=1}^n (X_i - \hat{\mu}_{y_i})(X_i - \hat{\mu}_{y_i})^T
$$
å…¶ä¸­ $n$ æ˜¯æ€»æ ·æœ¬æ•°ï¼Œ$K$ æ˜¯ç±»åˆ«æ•°ï¼Œ$\hat{\mu}_{y_i}$ æ˜¯ $X_i$ æ‰€å±ç±»åˆ«çš„å‡å€¼ã€‚
è®¡ç®—æ¯ä¸ªæ ·æœ¬åç¦»å…¶ç±»åˆ«å‡å€¼çš„é‡ï¼š

- ç±»åˆ« 1 çš„æ ·æœ¬ï¼š
$$
  X_1 = \begin{pmatrix} 5.1 \\ 3.5 \end{pmatrix} - \begin{pmatrix} 5.0 \\ 3.23 \end{pmatrix} = \begin{pmatrix} 0.1 \\ 0.27 \end{pmatrix}
$$
$$
  X_2 = \begin{pmatrix} 4.9 \\ 3.0 \end{pmatrix} - \begin{pmatrix} 5.0 \\ 3.23 \end{pmatrix} = \begin{pmatrix} -0.1 \\ -0.23 \end{pmatrix}
$$
$$
  X_3 = \begin{pmatrix} 5.0 \\ 3.2 \end{pmatrix} - \begin{pmatrix} 5.0 \\ 3.23 \end{pmatrix} = \begin{pmatrix} 0.0 \\ -0.03 \end{pmatrix}
$$

- ç±»åˆ« 2 çš„æ ·æœ¬ï¼š
$$
  X_4 = \begin{pmatrix} 6.1 \\ 2.9 \end{pmatrix} - \begin{pmatrix} 6.3 \\ 3.07 \end{pmatrix} = \begin{pmatrix} -0.2 \\ -0.17 \end{pmatrix}
$$$$
  X_5 = \begin{pmatrix} 6.3 \\ 3.3 \end{pmatrix} - \begin{pmatrix} 6.3 \\ 3.07 \end{pmatrix} = \begin{pmatrix} 0.0 \\ 0.23 \end{pmatrix}
$$$$
  X_6 = \begin{pmatrix} 6.5 \\ 3.0 \end{pmatrix} - \begin{pmatrix} 6.3 \\ 3.07 \end{pmatrix} = \begin{pmatrix} 0.2 \\ -0.07 \end{pmatrix}
$$
ç„¶åï¼Œè®¡ç®—åæ–¹å·®çŸ©é˜µçš„å„é¡¹ä¹‹å’Œå¹¶é™¤ä»¥ $n - K = 6 - 2 = 4$ï¼š
$$
\hat{\Sigma} = \frac{1}{4} \sum_{i=1}^6 (X_i - \hat{\mu}_{y_i})(X_i - \hat{\mu}_{y_i})^T
$$
å°†æ¯ä¸ªå·®ä¹˜ä»¥å®ƒçš„è½¬ç½®ï¼Œå†å°†ç»“æœç›¸åŠ ï¼Œæœ€åé™¤ä»¥4ï¼Œå¯ä»¥å¾—åˆ°æœ€ç»ˆçš„åæ–¹å·®çŸ©é˜µï¼ˆè¿™é‡Œç•¥å»å…·ä½“æ•°å€¼è®¡ç®—ï¼‰

#### Linear Discriminant Analysis when p>1

æš‚ç•¥



### Quadratic Discriminant Analysis (QDA)

QDA å‡å®šæ¯ä¸ªç±»åˆ«éƒ½æœ‰è‡ªå·±çš„åæ–¹å·®çŸ©é˜µã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒå‡å®šæ¥è‡ªç¬¬ k ä¸ªç±»åˆ«çš„è§‚æµ‹å€¼çš„å½¢å¼ä¸º X âˆ¼ N (Î¼k, Î£k)ï¼Œå…¶ä¸­ Î£k æ˜¯ç¬¬ k ä¸ªç±»åˆ«çš„**åæ–¹å·®çŸ©é˜µ**ã€‚

> [å¦‚ä½•ç›´è§‚åœ°ç†è§£ã€Œåæ–¹å·®çŸ©é˜µã€ï¼Ÿ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/37609917)

- QDA å‡è®¾æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æœä»ä¸€ä¸ªç‰¹å®šå‡å€¼å’Œåæ–¹å·®çš„**å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ**ï¼ˆå³æ­£æ€åˆ†å¸ƒ+åæ–¹å·®çŸ©é˜µå˜æ¢ï¼‰
- å¤šå…ƒé«˜æ–¯åˆ†å¸ƒä¼šå¯¼è‡´è¾¹ç•Œä¸ºéçº¿æ€§
- QDA åŸºäºè´å¶æ–¯å®šç†ï¼Œé€šè¿‡æœ€å¤§åŒ–åéªŒæ¦‚ç‡æ¥å†³å®šæ ·æœ¬çš„ç±»åˆ«
- QDA ä½¿ç”¨çš„æ˜¯äºŒæ¬¡åˆ¤åˆ«å‡½æ•°ï¼ˆåŒ…å«äºŒæ¬¡é¡¹ã€ä¸€æ¬¡é¡¹å’Œå¸¸æ•°é¡¹ï¼‰ï¼Œè¿™ä¸ LDA ä¸­çš„çº¿æ€§åˆ¤åˆ«å‡½æ•°ä¸åŒï¼Œä»è€Œåœ¨å†³ç­–è¾¹ç•Œä¸Šå½¢æˆ**éçº¿æ€§å†³ç­–è¾¹ç•Œ**
- QDA åœ¨ç±»åˆ«ä¹‹é—´çš„åæ–¹å·®çŸ©é˜µå·®å¼‚è¾ƒå¤§æ—¶è¡¨ç°è‰¯å¥½ï¼Œå› æ­¤é€‚åˆç±»åˆ«ä¹‹é—´çš„**æ–¹å·®å’Œåæ–¹å·®å·®å¼‚è¾ƒå¤§çš„æ•°æ®é›†**

#### Discriminant functions åˆ¤åˆ«å‡½æ•°

Assume $(X \mid y = k) \sim N_p(\mu_k, \Sigma_k)$ for $k = 1, \ldots, K$, then
$$

\delta_k(X) = \log (\pi_k) - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (X - \mu_k)^T \Sigma_k^{-1} (X - \mu_k)

$$


#### Parameter estimation

The quadratic term of $X$ is now necessary. We estimate each parameter as follows:

- $\hat{\pi}_k = n_k / n$
- $\mu_k$ is estimated by the centroid in each class $k$.
$$
  \hat{\mu}_k = \frac{1}{n_k} \sum_{\{i : y_i = k\}} X_i
$$
- **$\Sigma_k$ is estimated by the sample covariance matrix in each class (diff from LDA)**
$$  \hat{\Sigma}_k = \frac{1}{n_k - 1} \sum_{\{i : y_i = k\}} (X_i - \hat{\mu}_k)(X_i - \hat{\mu}_k)^T
$$
#### LDA v.s. QDA

![](./SDSC5001.assets/file-20241110011938422.png)

- Left: The Bayes (purple dashed), LDA (black dotted), and QDA (green solid) decision boundaries for a **two-class problem with $\Sigma_1 = \Sigma_2$.** The shading indicates the QDA decision rule. **Since the Bayes decision boundary is linear, it is more accurately approximated by LDA than by QDA.** 
- Right: Details are as given in the left-hand panel, **except that $\Sigma_1 \neq \Sigma_2$**. **Since the Bayes decision boundary is non-linear, it is more accurately approximated by QDA than by LDA.**

### Comparison of Classification Methods

![](./SDSC5001.assets/file-20241123122715171.png)
- Scenario 1: æ•°æ®æ¥è‡ªå‡å€¼ä¸åŒçš„Normal distributionï¼Œæ»¡è¶³LDAå‡è®¾ï¼ˆç‹¬ç«‹æ­£æ€åˆ†å¸ƒï¼‰ï¼Œç”±äºæ»¡è¶³ç‹¬ç«‹æ€§ï¼Œæœ´ç´ è´å¶æ–¯è¡¨ç°å¾ˆå¥½ï¼ŒKNN è¡¨ç°ä¸ä½³çš„åŸå› æ˜¯ï¼Œå®ƒåœ¨æ–¹å·®æ–¹é¢ä»˜å‡ºçš„ä»£ä»·å¹¶æ²¡æœ‰è¢«åå·®çš„å‡å°‘æ‰€æŠµæ¶ˆã€‚QDA çš„è¡¨ç°ä¹Ÿæ¯” LDA å·®ï¼Œå› ä¸ºå®ƒæ¯”å¿…è¦çš„åˆ†ç±»å™¨æ›´çµæ´»ã€‚é€»è¾‘å›å½’çš„è¡¨ç°ç›¸å½“ä¸é”™ï¼Œå› ä¸ºå®ƒå‡å®šäº†çº¿æ€§å†³ç­–è¾¹ç•Œ
- Scenario 2: æ•°æ®åœ¨S1çš„æ¡ä»¶ä¸‹å­˜åœ¨-0.5çš„ç›¸å…³æ€§ï¼Œå¯ä»¥çœ‹å‡ºæœ´ç´ è´å¶æ–¯çš„æ•ˆæœå˜å·®äº†
- Scenario 3: æ•°æ®æ»¡è¶³tåˆ†å¸ƒï¼Œt åˆ†å¸ƒçš„å½¢çŠ¶ä¸æ­£æ€åˆ†å¸ƒç›¸ä¼¼ï¼Œä½†å®ƒå€¾å‘äºäº§ç”Ÿæ›´å¤šçš„æç«¯ç‚¹ï¼Œå…¶è¿åäº†LDAå’ŒQDAçš„å‡è®¾ï¼Œæ•ˆæœå˜å·®ï¼Œä¸”QDAçš„æ•ˆæœå˜å·®æ›´ä¸¥é‡
![](./SDSC5001.assets/file-20241123123410456.png)
- Scenario 4: The data were generated from a **normal distribution**, with a correlation of 0.5 between the predictors in the first class, and correlation of âˆ’0.5 between the predictors in the second class. QDAè¡¨ç°éå¸¸å¥½ï¼Œå› ä¸ºè¿™ä¸€è®¾ç½®ç¬¦åˆ QDA å‡è®¾ï¼ˆå¤šå…ƒæ­£å¤ªåˆ†å¸ƒå‡è®¾ï¼‰ï¼Œå¹¶äº§ç”Ÿäº†äºŒæ¬¡å†³ç­–è¾¹ç•Œã€‚
- Scenario 5: æ•°æ®ç”±ä¸ç›¸å…³é¢„æµ‹å› å­çš„æ­£æ€åˆ†å¸ƒç”Ÿæˆã€‚ç„¶åä»åº”ç”¨äºé¢„æµ‹å› å­çš„å¤æ‚éçº¿æ€§å‡½æ•°çš„å¯¹æ•°å‡½æ•°ä¸­å¯¹å“åº”è¿›è¡Œé‡‡æ ·ã€‚KNN-CVè¡¨ç°æœ€å¥½ï¼Œä½†æ˜¯KNN-1å¾ˆå·®ï¼Œå³ä½¿æ•°æ®å‘ˆç°å‡ºå¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œå¦‚æœå¹³æ»‘åº¦é€‰æ‹©ä¸å½“ï¼ŒKNN ç­‰éå‚æ•°æ–¹æ³•ä»ç„¶ä¼šå¾—åˆ°è¾ƒå·®çš„ç»“æœã€‚
- Scenario 6: æ•°æ®æ¥è‡ªnormal distributionï¼Œæ¯ç±»æ•°æ®å¸¦æœ‰ä¸åŒçš„å¯¹è§’åæ–¹å·®çŸ©é˜µæ§åˆ¶å°ºåº¦çš„å˜åŒ–ï¼Œä½†æ˜¯æ•°æ®é‡éå¸¸å°ã€‚ç”±äºæ»¡è¶³ç‹¬ç«‹æ€§ï¼Œæœ´ç´ è´å¶æ–¯æ•ˆæœè¾ƒå¥½ï¼Œä½†æ˜¯æ•°æ®é‡å°å¯¼è‡´QDAçš„æ•ˆæœä¸å¦‚æœ´ç´ è´å¶æ–¯ï¼Œä¸”KNN çš„æ€§èƒ½ä¹Ÿå—åˆ°äº†å½±å“ã€‚

#### Summary

- True boundaries are linear -> LDA and logistic regression
- True boundaries are moderately non-linear -> QDA or naive Bayes
- more complicated decision boundaries -> KNN
- skills -> can accommodate a non-linear relationship between the predictors and the response, such as $X^2$ $X^3$

### Generalized Linear Models

The problem using linear regression in the Bike dataset:
![](./SDSC5001.assets/file-20241123151505143.png)
- Between 1 AM and 4 AM, æ— è®ºå¤©æ°”/æœˆä»½å¦‚ä½•ï¼Œä½¿ç”¨bikeçš„äººæ•°æ€»æ˜¯å°‘çš„ï¼Œæ–¹å·®å°‘
- By contrast, between 7 AM and 10 AM, in April, May, and June, å¤©æ°”å¥½ï¼Œå¤§å®¶å€¾å‘äºéª‘è½¦; in December, January, and February, å¤©æ°”éå¸¸ç³Ÿç³•ï¼Œå¾ˆå°‘äººä¼šéª‘è½¦, æ­¤æ—¶åœ¨åŒä¸€æ—¶é—´ä¸­å¯èƒ½å‡ºç°å¾ˆå¤§çš„å·®å¼‚ï¼Œæ–¹å·®éå¸¸å¤§
- è¿åäº†Linear Regressionä¸­çš„æ–¹å·®ä¸€è‡´æ€§ï¼ŒHeteroscedasticity
- ä¸”`bike`å˜é‡å¿…é¡»è¾“å‡ºæ•´æ•°ï¼Œè€Œä¸èƒ½å°æ•°

By transforming the response using:

$$
\log(Y)=\sum_{j=1}^p X_j \beta_j+\epsilon
$$
can overcome much of the heteroscedasticity in the untransformed data. But it will lead to hard interpretation. And if the respond value is 0, log function cannot be applied.

#### Poisson Regression on the Bikeshare Data

$$
\Pr(Y = k) = \frac{e^{-\lambda} \lambda^k}{k!} \quad \text{for } k = 0, 1, 2, \dots.
$$
$\lambda = \mathbb{E}(Y) = \mathrm{Var}(Y)$ This means that if Y follows the Poisson distribution, then the l**arger the mean of Y , the larger its variance**.

$$
\log(\lambda(X_1, \dots, X_p)) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
$$
### ROC curve

A Confusion matrix

|                   | **Positive (é¢„æµ‹)** | **Negative (é¢„æµ‹)** |
| ----------------- | ----------------- | ----------------- |
| **Positive (çœŸå®)** | TP<br>            | FN                |
| **Negative (çœŸå®)** | FP                | TN                |
æŠ€å·§ï¼šå­—æ¯çš„ç¬¬äºŒä½éƒ½æ˜¯è¡¨ç¤º**é¢„æµ‹å€¼**(P/N)ï¼Œç¬¬ä¸€ä½éƒ½æ˜¯è¡¨ç¤ºé¢„æµ‹å€¼çš„æ­£ç¡®ä¸å¦(T/F)ï¼Ÿ

ä¾‹å¦‚ï¼šFPä»£è¡¨  P->é¢„æµ‹æ˜¯Positiveï¼›F->é¢„æµ‹é”™è¯¯ï¼Œæ‰€ä»¥çœŸå®å€¼æ˜¯Negative

For TP, FN, FP, TN
- TP+FN=1
- FP+TN=1
- **å¯¹äºåŒä¸€ä¸ªç³»ç»Ÿæ¥è¯´ï¼Œè‹¥TPå¢åŠ ï¼Œåˆ™FPä¹Ÿå¢åŠ **
	- å¯¹äºä¸€ä¸ªç³»ç»Ÿæ¥è¯´ï¼Œé¢„æµ‹æ­£ç¡®çš„é˜ˆå€¼ä¸‹é™äº†ï¼Œè™½ç„¶TPæ ·æœ¬å¢åŠ äº†ï¼Œä½†æ˜¯åŸæœ¬æ˜¯Negativeçš„æ ·æœ¬ä¹Ÿä¼šæ›´å®¹æ˜“è¢«é¢„æµ‹ä¸ºPositiveï¼

![](./SDSC5001.assets/file-20241207202858801.png)
ROCæ›²çº¿æ˜¯æ¨ªåæ ‡ä¸ºFPï¼Œçºµåæ ‡ä¸ºTPï¼Œå–ä¸åŒé˜ˆå€¼çš„æ›²çº¿

å¯ä»¥å‘ç°ï¼Œè¶Šå¾€å·¦ä¸Šåç½®çš„æ›²çº¿ï¼Œåœ¨å–å¾—è¾ƒå¥½çš„FPï¼ˆé¢„æµ‹ä¸ºPositiveä¸”æ­£ç¡®ï¼‰æ—¶FPè¾ƒä½ï¼ˆé¢„æµ‹ä¸ºPositiveä½†é”™è¯¯ï¼‰ï¼Œæ­¤æ—¶æ¨¡å‹æœ‰è¾ƒå¥½çš„æ€§èƒ½

ROCçš„é¢ç§¯ä¸ºAUCæŒ‡æ ‡

> è¿˜æœ‰ä¸€ç§æŒ‡æ ‡ä¸ºEERï¼ˆç­‰é”™è¯¯ç‡ Equal Error Rateï¼‰ï¼Œå³å½“ä¸¤ç±»é”™è¯¯FPï¼ˆé¢„æµ‹ä¸ºPositiveä½†é”™è¯¯ï¼‰å’ŒFNï¼ˆé¢„æµ‹ä¸ºNegativeä½†é”™è¯¯ï¼‰ç›¸ç­‰çš„æ—¶å€™çš„é”™è¯¯ç‡ï¼ˆæˆ–è€…æ¯”è¾ƒTPçš„å€¼ï¼‰


## Resampling Method

### Cross-Validation

#### LOOCV

![](./SDSC5001.assets/file-20241124031846573.png)



#### k-Fold Cross-Validation

![](./SDSC5001.assets/file-20241124031748633.png)

é€šè¿‡éšæœº k æŠ˜ CV å°†è§‚æµ‹æ•°æ®é›†åˆ†æˆå¤§å°å¤§è‡´ç›¸åŒçš„ k ç»„ï¼ˆæˆ–æŠ˜å ï¼‰ã€‚ç¬¬ä¸€ä¸ªæŠ˜å è¢«è§†ä¸ºéªŒè¯é›†ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºå…¶ä½™çš„ k - 1 ä¸ªæŠ˜å ã€‚ç„¶å**å¯¹ä¿ç•™çš„æŠ˜å ä¸­çš„è§‚æµ‹å€¼è®¡ç®—å‡æ–¹è¯¯å·®** $MSE_1,...,MES_2,...,MSE_k$. The test error is estimated by averaging these MSE estimates.
$$
\text{CV}(k) = \frac{1}{k} \sum_{i=1}^k \text{MSE}_i.
$$
In practice, one typically performs **k-fold CV using k = 5 or k = 10**. What is the advantage of using k = 5 or k = 10 rather than k = n? The most obvious advantage is **computational.**

![](./SDSC5001.assets/file-20241124030453434.png)

çœŸå®æµ‹è¯• MSE æ˜¾ç¤ºä¸ºè“è‰²ï¼ŒLOOCV ä¼°è®¡å€¼æ˜¾ç¤ºä¸ºé»‘è‰²è™šçº¿ï¼Œ10 å€ CV ä¼°è®¡å€¼æ˜¾ç¤ºä¸ºæ©™è‰²ã€‚Xè¡¨ç¤ºæ¯æ¡ MSE æ›²çº¿çš„æœ€å°å€¼ï¼ˆå¹¶ä¸”å¯¹åº”ä¸€ä¸ªflexibilityå€¼ï¼‰ã€‚å°½ç®¡å®ƒä»¬æœ‰æ—¶ä¼šä½ä¼°çœŸå®çš„æµ‹è¯• MSEï¼Œä½†æ‰€æœ‰çš„ CV æ›²çº¿éƒ½æ¥è¿‘äºç¡®å®šæ­£ç¡®çš„flexibilityå€¼ï¼Œå³ CVå¾—åˆ°çš„æœ€å°MSEï¼ˆæ©™è‰²Xï¼‰å¯¹åº”çš„flexibility ä¸ çœŸå®æœ€å°MSEï¼ˆè“è‰²Xï¼‰å¯¹åº”çš„flexibilityæ˜¯ç›¸è¿‘çš„ã€‚

ç”±äºLOOCVä½¿ç”¨äº†æ›´å¤šæ•°æ®è¿›è¡Œæ‹Ÿåˆï¼Œbiasä¼šæ¯”fold-CVå°ï¼Œè€Œæ–¹å·®ä¼šæ›´å¤§ã€‚**ç”¨ k = 5 æˆ– k = 10 æ¥æ‰§è¡Œ k å€äº¤å‰éªŒè¯**ï¼Œç»éªŒè¡¨æ˜ï¼Œè¿™äº›å€¼äº§ç”Ÿçš„æµ‹è¯•è¯¯å·®ç‡ä¼°è®¡å€¼æ—¢ä¸ä¼šå‡ºç°è¿‡é«˜çš„åå·®ï¼Œä¹Ÿä¸ä¼šå‡ºç°è¿‡é«˜çš„æ–¹å·®ã€‚

#### CV on Classification Problems
![](./SDSC5001.assets/file-20241124035020393.png)

![](./SDSC5001.assets/file-20241124035009301.png)

ä¸Šå›¾ï¼šTest error (brown), **training error (blue)**, and **10-fold CV error (black)** on the two-dimensional classification data

å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¾ˆéš¾é€‰æ‹©é€»è¾‘å›å½’çš„å¤šé¡¹å¼é˜¶æ•°ï¼Œ
$$
\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + ... +\beta_? X_1^? + \beta_3 X_2 + \beta_4 X_2^2+... +\beta_? X_2^?.
$$
å¯¹äºKNNï¼ŒKçš„é€‰æ‹©ä¹Ÿè¾ƒå›°éš¾

**å¦‚æœä½¿ç”¨training erroræ¥è¯„ä¼°å‚æ•°ä¼šå¤±çœŸ**ï¼ˆè“è‰²çš„çº¿ä¸é»„è‰²çš„çº¿**ä¸ä¸€è‡´**ï¼‰ï¼Œå¯èƒ½å› ä¸ºè¿‡æ‹Ÿåˆï¼ˆé«˜é˜¶å¤šé¡¹å¼å’Œè¾ƒå°çš„Kï¼‰

æ­¤æ—¶å¯ä»¥åˆ©ç”¨CVæ¥ä¼°è®¡éªŒè¯é›†çš„è¯¯å·®æ›²çº¿ï¼Œé€‰å‡ºä¸€ä¸ªè¯¯å·®å€¼æœ€ä½çš„è¶…å‚æ•°ï¼ˆå›¾ä¸­é»‘çš„çº¿ä¸é»„è‰²çš„çº¿**ä¸€è‡´**ï¼Œå¯ä»¥åˆ©ç”¨CVçš„æœ€å°å€¼é»‘è‰²çš„Xæ¥å¯»æ‰¾è¶…å‚æ•°ï¼‰

### Bootstrap

æ–¹å·®ä¼°è®¡çš„é—®é¢˜ï¼š
1. ä¼ ç»Ÿçš„æ ·æœ¬ç»Ÿè®¡æ–¹æ³•é€šå¸¸ä¾èµ–äºæŸäº›åˆ†å¸ƒå‡è®¾ï¼ˆä¾‹å¦‚æ­£æ€åˆ†å¸ƒï¼‰æ¥ä¼°è®¡æ€»ä½“æ–¹å·®ï¼Œå¦‚æœè¿™äº›å‡è®¾ä¸æˆç«‹ï¼ˆå¦‚æ€»ä½“åˆ†å¸ƒæœªçŸ¥æˆ–åç¦»æ­£æ€åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆç›´æ¥åˆ©ç”¨æ ·æœ¬æ–¹å·®ä¼°è®¡çš„ç»“æœå¯èƒ½æ˜¯ä¸å‡†ç¡®çš„
2. å¦‚æœç›´æ¥è®¡ç®—æ ·æœ¬çš„æ–¹å·®Â  $S^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2$ å—å•ä¸ªæ ·æœ¬ç‚¹çš„å½±å“è¾ƒå¤§

**Bootstrap** çš„ä¸»è¦ç›®çš„æ˜¯è¡¡é‡ç»Ÿè®¡é‡ï¼ˆå¦‚å‡å€¼ã€æ–¹å·®ã€å›å½’ç³»æ•°ç­‰ï¼‰åœ¨ç‰¹å®šæ ·æœ¬ä¸­çš„**å˜å¼‚æ€§/æ³¢åŠ¨æ€§ï¼ˆvariabilityï¼‰**

å‡è®¾ï¼š
1. æ ·æœ¬æ˜¯ä»æ€»ä½“ä¸­ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰æŠ½å–çš„ï¼Œä»£è¡¨äº†æ€»ä½“çš„åˆ†å¸ƒã€‚
2. é€šè¿‡å¯¹æ ·æœ¬åå¤é‡é‡‡æ ·ï¼Œå¯ä»¥æ¨¡æ‹Ÿç»Ÿè®¡é‡åœ¨æ€»ä½“ä¸­çš„å˜å¼‚æ€§ã€‚

![](./SDSC5001.assets/file-20241124065312999.png)

$$
\text{SE}_B(\hat{\alpha}) = \sqrt{\frac{1}{B - 1} \sum_{r=1}^B \left( \hat{\alpha}^{*r} - \frac{1}{B} \sum_{r'=1}^B \hat{\alpha}^{*r'} \right)^2}.
$$

## Linear Model Selection and Regularization

OLS is not the best in some situations. As we will see, alternative fitting procedures can yield better prediction accuracy and model interpretability.

### Subset Selection

#### Best Subset Selection

ä¸ºäº†è¿›è¡Œæœ€ä½³å­é›†é€‰æ‹©ï¼Œæˆ‘ä»¬å¯¹ p ä¸ªé¢„æµ‹å› å­(**predictors**/datasets features)çš„æ¯ç§å¯èƒ½ç»„åˆåˆ†åˆ«è¿›è¡Œæœ€å°äºŒä¹˜å›å½’æœ€ä½³å­é›†é€‰æ‹©.
æˆ‘ä»¬æ‹¥æœ‰
$$
\binom{p}{2} = \frac{p(p-1)}{2}
$$
ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”æ‰¾å‡ºæœ€ä½³çš„æ¨¡å‹

![](./SDSC5001.assets/file-20241124071329816.png)
æˆ‘ä»¬çŸ¥é“ï¼Œç”±äº**predictors**çš„æ•°é‡å¢åŠ ï¼Œæ¨¡å‹çš„RSSä¼šé™ä½ï¼Œ$R^2$ä¼šæå‡ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨$adjusted\ R^2$ BIC ç­‰æŒ‡æ ‡ï¼Œå¹¶ä¸”è€ƒè™‘éœ€è¦ä½¿ç”¨CVè¡¡é‡average test error.

æœ€ä½³å­é›†é€‰æ‹©åœ¨è®¡ç®—ä¸Šä¸å¯è¡Œ

#### Stepwise Selection

![](./SDSC5001.assets/file-20241124134927113.png)

å‰å‘é€æ­¥é€‰æ‹©ä»ä¸€ä¸ªä¸åŒ…å«ä»»ä½•é¢„æµ‹å› å­çš„æ¨¡å‹å¼€å§‹ï¼Œç„¶åé€æ¬¡å‘æ¨¡å‹ä¸­æ·»åŠ é¢„æµ‹å› å­ï¼Œç›´åˆ°æ¨¡å‹ä¸­åŒ…å«æ‰€æœ‰é¢„æµ‹å› å­

![](./SDSC5001.assets/file-20241124135232569.png)é€†å‘é€æ­¥é€‰æ‹©ä»åŒ…å«æ‰€æœ‰ p ä¸ªé¢„æµ‹å› å­çš„å…¨æœ€å°äºŒä¹˜æ¨¡å‹å¼€å§‹ï¼Œç„¶åé€æ¬¡è¿­ä»£å»é™¤æœ€æ— ç”¨çš„é¢„æµ‹å› å­ã€‚

åå‘é€‰æ‹©è¦æ±‚æ ·æœ¬æ•° n å¤§äºå˜é‡æ•° pï¼ˆè¿™æ ·æ‰èƒ½æ‹Ÿåˆå‡ºå®Œæ•´çš„æ¨¡å‹ï¼‰

Stepwise Selection methods are not guaranteed to yield the best model containing a subset of the $p$ predictors

#### Choosing the Optimal Model

ç”±äºOSLå°½é‡ä½¿å¾—è®­ç»ƒé›† $MSE$ é™ä½ï¼Œè€Œä¸æ˜¯æµ‹è¯• $MSE$, è®­ç»ƒé›† $MSE$ é€šå¸¸ä¼šä½ä¼°æµ‹è¯• $MSE$ (ä¾‹å¦‚è¿‡æ‹Ÿåˆå¯¼è‡´æ®‹å·®é™ä½ï¼Œè®­ç»ƒ$MSE$é™ä½)

æ‰€ä»¥è®­ç»ƒé›† $RSS$ å’Œè®­ç»ƒé›† $R^2$ ä¸èƒ½ç”¨äºä»å˜é‡æ•°é‡ä¸åŒçš„æ¨¡å‹ä¸­è¿›è¡Œé€‰æ‹©

However, a number of techniques for **adjusting** the training error for the model size are available. These approaches can be used to select among a set of models with different numbers of variables. We now consider four such approaches: $C_p$, **Akaike information criterion** (AIC), **Bayesian information criterion** (BIC), and **adjusted $R^2$. 

$$
C_p = \frac{1}{n} \left( \text{RSS} + 2d\hat{\sigma}^2 \right)
$$
$$
\text{AIC} = \frac{1}{n} \left( \text{RSS} + 2d\hat{\sigma}^2 \right)
$$
$$
\text{BIC} = \frac{1}{n} \left( \text{RSS} + \log(n)d\hat{\sigma}^2 \right)
$$
- $d$: the number of predictor
- $\sigma^2$: estimate of the variance of the error
- $2d\hat{\sigma}^2$: å¯¹predictoræ•°é‡çš„å¢åŠ å’Œvarianceçš„æƒ©ç½šé¡¹
- $n$: the number of observations.

BIC é€šå¸¸ä¼šå¯¹**å˜é‡è¾ƒå¤šçš„æ¨¡å‹æ–½åŠ è¾ƒé‡çš„æƒ©ç½š**, å€¾å‘äºé€‰æ‹©å°æ¨¡å‹

![](./SDSC5001.assets/file-20241124145022210.png)
$C_p$, $BIC$, and adjusted $R_2$ are shown for the best models (Xä½ç½®) of each size for the Credit data set, å¯ä»¥å¾ˆæ˜æ˜¾å‘ç°BICå€¾å‘äºé€‰æ‹©å°æ¨¡å‹, ä¸”BICåœ¨æœ€å¤§æœ€ä½å€¼åé¢æœ‰æ˜æ˜¾çš„ä¸Šå‡ï¼Œè€Œ$C_p$å’ŒAdjusted $R^2$è¶‹äºå¹³ç¼“

K-flod CV ä¹Ÿå¯ä»¥å¾ˆå¥½çš„è¿›è¡Œsubseté€‰æ‹©

### Shrinkage Methods

#### Ridge Regression

Ridge Regressionï¼ˆ**å²­å›å½’**ï¼‰æ˜¯ä¸€ç§æ”¹è‰¯çš„æœ€å°äºŒä¹˜ä¼°è®¡æ³•ï¼Œé€šè¿‡æ”¾å¼ƒæœ€å°äºŒä¹˜æ³•çš„æ— åæ€§ï¼Œä»¥æŸå¤±éƒ¨åˆ†ä¿¡æ¯ã€é™ä½ç²¾åº¦ä¸ºä»£ä»·è·å¾—å›å½’ç³»æ•°æ›´ä¸ºç¬¦åˆå®é™…ã€æ›´å¯é çš„å›å½’æ–¹æ³•ï¼Œå¯¹ç—…æ€æ•°æ®çš„æ‹Ÿåˆè¦å¼ºäºæœ€å°äºŒä¹˜æ³•

minimize the quantity:
$$
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p \beta_j^2 = \text{RSS} + \lambda \sum_{j=1}^p \beta_j^2
$$
- $\lambda$: tuning parameter
- $\beta_j^2$: L2 normalisation
- $\lambda \sum_{j=1}^p \beta_j^2$: shrinkage penalty

![](./SDSC5001.assets/file-20241124152304721.png)

å¹³æ–¹åå·®ï¼ˆé»‘è‰²ï¼‰ã€æ–¹å·®ï¼ˆç»¿è‰²ï¼‰å’Œæµ‹è¯•å‡æ–¹è¯¯å·®ï¼ˆç´«è‰²ï¼‰
$\frac{\|\hat{\beta}^\text{R}_\lambda\|_2}{\|\hat{\beta}\|_2}$è¡¨ç¤ºRidge Regressionçš„$\beta$ ç³»æ•°å’ŒOLSä¸‹çš„$\beta$ç³»æ•°çš„æ¯”å€¼ï¼Œè¡¨ç¤ºéšç€æ­£åˆ™åŒ–å‚æ•° $\lambda$ çš„å˜åŒ–ï¼Œæ­£åˆ™åŒ–æ¨¡å‹ä¸­ç³»æ•°çš„ç¼©å°ç¨‹åº¦

#### Lasso

minimize the quantity:
$$
\sum_{i=1}^n \left( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^p |\beta_j| = \text{RSS} + \lambda \sum_{j=1}^p |\beta_j|
$$
ä¸ Ridge Regression çš„åŒºåˆ«åœ¨äºä½¿ç”¨L1 Norm

![](./SDSC5001.assets/file-20241124152957319.png)

ä½¿ç”¨L1èŒƒæ•°æ›´å®¹æ˜“ä½¿å¾—è¯¯å·®ç­‰é«˜çº¿è§¦åŠè¾¹ç¼˜ä½ç½®ï¼Œå³$\beta_?=0$çš„ä½ç½®ï¼Œè¿™ä½¿å¾—æŸäº›predictorçš„ç³»æ•°å˜ä¸º0ï¼Œå…·æœ‰é™ç»´çš„æ•ˆæœ

![](./SDSC5001.assets/file-20241124153639131.png)
å¯ä»¥å‘ç°åœ¨æƒ©ç½šç³»æ•°å¢å¤§æ—¶ï¼ŒæŸäº›predictorç³»æ•°ç›´æ¥æ¶ˆå¤±äº†

#### Ridge Regression vs. Lasso

**Lasso**: 
- å¦‚æœå“åº”å˜é‡ä¸»è¦å—å°‘æ•°å‡ ä¸ªé‡è¦é¢„æµ‹å˜é‡é©±åŠ¨ï¼Œè€Œå…¶ä»–å˜é‡çš„å½±å“å¯ä»¥å¿½ç•¥ä¸è®¡
- å½“ç‰¹å¾æ•°é‡è¿œå¤§äºæ ·æœ¬é‡ï¼ˆ$p \gg n$ï¼‰æ—¶ï¼ŒLasso å¯ä»¥å¸®åŠ©é€‰æ‹©å¯¹å“åº”å˜é‡å½±å“è¾ƒå¤§çš„å°‘é‡ç‰¹å¾
**Ridge Regression**:
- é«˜ç»´æ•°æ®ï¼ˆ$p > n$ï¼‰æˆ–å¤šé‡å…±çº¿æ€§é—®é¢˜Multicollinearity: å¯¹ç³»æ•°è¿›è¡Œå‡åŒ€æ”¶ç¼©ï¼Œä»è€Œå‡å°å…±çº¿æ€§çš„å½±å“
- å½“æ‰€æœ‰å˜é‡çš„çœŸå®ç³»æ•°è¾ƒå°ä¸”åˆ†å¸ƒå‡åŒ€æ—¶ï¼ˆå³æ²¡æœ‰æ˜¾è‘—çš„å¤§ç³»æ•°ï¼‰ï¼ŒRidge èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å…¨å±€ä¿¡æ¯
**Elastic Net**:
 $$
\text{Elastic Net penalty} = \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2
$$
Cross-validation can be used in order to determine which approach is better on a particular data set.

### Dimension Reduction Methods

å®šä¹‰äº†æ–°çš„å˜é‡Â  $Z_1, Z_2, \dots, Z_M$ ï¼Œå®ƒä»¬æ˜¯åŸå§‹é¢„æµ‹å˜é‡Â  $X_1, X_2, \dots, X_p$Â  çš„çº¿æ€§ç»„åˆ
$$
Z_m = \sum_{j=1}^p \phi_{jm} X_j, \quad m = 1, \dots, M
$$
åŸºäºæ–°æ„é€ çš„å˜é‡Â  $Z_1, Z_2, \dots, Z_M$Â  æ„å»ºçº¿æ€§å›å½’æ¨¡å‹
$$
y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i, \quad i = 1, \dots, n
$$
é€šè¿‡åˆç†åœ°é€‰æ‹©Â  $\phi_{jm}$ï¼Œé™ä½äº†éœ€è¦ä¼°è®¡çš„å‚æ•°ä¸ªæ•°ï¼Œä»Â  p+1Â  é™åˆ°Â  M+1 ï¼Œå‡å°‘äº†è®¡ç®—å¤æ‚æ€§

#### PCA / PCR

PCA(Principal Components Analysis) æ˜¯ PCR(Principal Component Regression)çš„å‰ç½®å¤„ç†æ­¥éª¤,æå– $Z_1, Z_2, \dots, Z_M$ ä½œä¸ºä¸»æˆåˆ†

ç¬¬ä¸€ä¸ªä¸»æˆåˆ†æ˜¯åŸå§‹ç‰¹å¾Â  $X_1, X_2, \dots, X_p$Â  çš„çº¿æ€§ç»„åˆ
$$
Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1} X_p
$$
å…¶ä¸­ï¼Œ$\phi_{j1}$ æ˜¯ç¬¬ä¸€ä¸»æˆåˆ†çš„è½½è·ï¼ˆloadingï¼‰ï¼Œè¡¨ç¤ºç¬¬Â $j$ ä¸ªåŸå§‹ç‰¹å¾å¯¹ç¬¬ä¸€ä¸»æˆåˆ†çš„è´¡çŒ®

ä¸»æˆåˆ†çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿æ•°æ®æŠ•å½±åçš„æ ·æœ¬æ–¹å·®æœ€å¤§çš„çº¿æ€§ç»„åˆ
$$
\max_{\phi_{11}, \dots, \phi_{p1}} \left\{ \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j1} x_{ij} \right)^2 \right\} 
\text{ subject to }   
\sum_{j=1}^p \phi_{j1}^2 = 1
$$
è®¡ç®—æ—¶æˆ‘ä»¬ä½¿ç”¨ç‰¹å¾å€¼åˆ†è§£æ³•è§£å†³ä¸Šè¿°ä¼˜åŒ–é—®é¢˜ï¼Œå³ï¼š
1. ä¸­å¿ƒåŒ–
2. è®¡ç®—åæ–¹å·®çŸ©é˜µ $\Sigma = \frac{1}{n} X^T X  = Q \Lambda Q^T$
	- QÂ æ˜¯ç‰¹å¾å‘é‡çŸ©é˜µï¼ˆåˆ—å‘é‡å¯¹åº”ä¸»æˆåˆ†æ–¹å‘ï¼‰
	- $\Lambda$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä¸ºç‰¹å¾å€¼ï¼Œä»£è¡¨æ¯ä¸ªä¸»æˆåˆ†çš„è§£é‡Šæ–¹å·®å¤§å°
3. ä»Qä¸­å–å‰Mä¸ªè¾ƒå¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ„æˆæŠ•å½±çŸ©é˜µ $Q_{\text{reduced}}$
4. å°†åŸå§‹æ•°æ®æŠ•å½±åˆ°å‰ä¸¤ä¸ªä¸»æˆåˆ†æ–¹å‘ä¸Š $Z = X \cdot Q_{\text{reduced}}$

PCAè™½ç„¶å¯ä»¥é€šè¿‡**çº¿æ€§ç»„åˆ**åŸå§‹å˜é‡æå–ä¸»æˆåˆ†ï¼Œä½†å®ƒå¹¶ä¸èƒ½çœŸæ­£è¢«è§†ä¸ºç‰¹å¾æå–æ–¹æ³•ï¼Œåªèƒ½çœ‹åšä¸€ç§æ•°æ®å‹ç¼©çš„æ–¹æ³•ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œä¸»æˆåˆ†åˆ†ææ¥è¿‘å²­å›éš™(L2 norm), è€Œä¸æ˜¯LASSO(L1 norm)

When performing PCR, we generally recommend **standardizing each predictor** to ensure all variables are on the same scale, prior to generating the principal components.

#### Partial Least Squares

PCA is an **unsupervised way,** since the response Y is not used to help determine the principal component directions.

Consequently, **PCR** suffers from a drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.

PLS is a supervised alternative to PCR.

### Considerations in High Dimensions

The issue of high dimensions
- OSL should not be performed
- `mse=0` but useless at all
- easy to overfitting
- $R^2$ increases to 1 as the number of features included

## Moving Beyond Linearity

### Polynomial Regression


### Step Functions

å°† X çš„å–å€¼èŒƒå›´åˆ†ä¸º K ä¸ª**åˆ‡åˆ†ç‚¹ (cutpoints)** $c_1, c_2, \dots, c_K$ï¼Œè¿™äº›åˆ‡åˆ†ç‚¹å°† $X$ çš„èŒƒå›´åˆ’åˆ†ä¸º $K+1$ ä¸ªåŒºé—´
ä¸ºæ¯ä¸ªåŒºé—´å®šä¹‰ K+1 ä¸ªæ–°å˜é‡ï¼š
- $C_0(X) = I(X < c_1)$
- $C_1(X) = I(c_1 \leq X < c_2)$
- $\dots$
- $C_{K-1}(X) = I(c_{K-1} \leq X < c_K)$
- $C_K(X) = I(c_K \leq X)$

å…¶ä¸­ï¼Œ$I(\cdot)$ æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œå€¼ä¸º 1 è¡¨ç¤ºæ¡ä»¶ä¸ºçœŸï¼Œå€¼ä¸º 0 è¡¨ç¤ºæ¡ä»¶ä¸ºå‡

åˆ©ç”¨OSLæ¥æ‹Ÿåˆæ¨¡å‹ï¼š
$$
f(X) = \sum_{k=0}^K \beta_k C_k(X)
$$
### Basis Functions

- Polynomial Regression and Step Functions are special cases of a *basis function* approach
$$
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2 b_2(x_i) + \beta_3 b_3(x_i) + \cdots + \beta_K b_K(x_i) + \epsilon_i
$$
- $b_k(x_i)$ are fixed and known

### Regression Splines

#### Piecewise Polynomials


*piecewise polynomial* regression involves fitting separate low-degree polynomials **over different regions of X**.

$$
y_i =
\begin{cases}
\beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 + \beta_{31}x_i^3 + \epsilon_i & \text{if } x_i < c, \\
\beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 + \beta_{32}x_i^3 + \epsilon_i & \text{if } x_i \geq c.
\end{cases}
$$
- $c$ï¼šåˆ‡åˆ†ç‚¹ï¼Œç”¨äºåˆ†éš”åŒºé—´

#### Constraints and Splines

![](./SDSC5001.assets/file-20241202065633684.png)

å›¾ä¸­åˆ©ç”¨**ä¸‰**æ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œæ¯å¢åŠ ä¸€ä¸ªåˆ†å‰²ç‚¹å¢åŠ **4**ä¸ªè‡ªç”±åº¦(è€ƒè™‘$\epsilon_i$)

The top left panel of Figure looks **wrong** because the fitted curve is just too flexible. 
**Solve:** add constraint that the fitted curve must be continuous.
Method:
- Top Left: constrained to be continuous at age=50
- Bottom Leftï¼šcontinuous value, same continuous first and second derivatives at age=50
  é™å®šå¤šé¡¹å¼åœ¨ age = 50 è¿ç»­, ä¸€é˜¶å¯¼æ•°å’ŒäºŒé˜¶å¯¼æ•°éƒ½å­˜åœ¨
- Bottom Rightï¼š**A linear spline**

æ¯å¢åŠ ä¸€ä¸ªé™åˆ¶ï¼Œå‡å°‘ä¸€ä¸ªè‡ªç”±åº¦

**linear spline:** å®ƒåœ¨age=50å¤„è¿ç»­ã€‚dé˜¶æ ·æ¡çš„ä¸€èˆ¬å®šä¹‰æ˜¯åˆ†æ®µ $d$ é˜¶å¤šé¡¹å¼ï¼ŒåŒæ—¶åœ¨æ¯ä¸ªç»“ç‚¹ç›´åˆ° $d-1$ é˜¶å¯¼æ•°éƒ½æ˜¯è¿ç»­çš„ã€‚å› æ­¤ï¼Œçº¿æ€§æ ·æ¡å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¾—åˆ°ï¼šåœ¨æ¯ä¸ªåŒºåŸŸå†…æ‹Ÿåˆä¸€æ¡ç›´çº¿ï¼ŒåŒæ—¶è¦æ±‚åœ¨å„ç»“ç‚¹å¤„æ»¡è¶³è¿ç»­æ€§ã€‚

#### The Spline Basis Representation

ä¸‰æ¬¡æ ·æ¡(cubic spline)ï¼šè¯¥æ¨¡å‹ä½¿ç”¨ä¸‰æ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œå¯¹äºKä¸ªåˆ†å‰²ç‚¹ $\xi_k$ï¼Œåœ¨åŸæœ‰Basis Functionsè¡¨è¾¾å¼ä¸­å¢åŠ Kä¸ªæˆªæ–­å¹‚åŸº truncated power basis å‡½æ•°ï¼Œå³ï¼š
$$
y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3 + \sum_{k=4}^{K+3} \beta_k h(X_i, \xi_k) + \epsilon_i
$$
$$
h(x, \xi) = (x - \xi)^3_+ =
\begin{cases}Â 
(x - \xi)^3 & \text{if } x > \xi \\
0 & \text{otherwise}
\end{cases}
$$

- $\beta_? h(X_i, \xi_?)$çš„ä¸ªæ•°å–å†³äºåˆ†å‰²ç‚¹Kçš„æ•°é‡
- å¯ä»¥è¯æ˜ï¼Œå…¶ä¼šå¯¼è‡´å¯¹åº”çš„ç»“ç‚¹çš„å‡½æ•°å€¼ã€ä¸€é˜¶/äºŒé˜¶å¯¼æ•°è¿ç»­
- $y_i$è‡ªç”±åº¦çš„å¤§å°ä¸º $4+K$ï¼ˆ$\beta$çš„æ•°é‡ï¼‰
 
 å¯¹äºnæ¬¡æ ·æ¡ï¼š
 - å°†basis for a cubic polynomialçš„ä¸ªæ•°å¢åŠ ä¸ºn, å³:
 $\beta_0 + \beta_1 X_i + \cdots + \beta_3 X_i^n$
 - truncated power basis function is defined as:
 $h(x, \xi) = (x - \xi)^n_+$

## Classification and Regression Trees

### Regression Tree

#### building a regression tree.

1. Divide the predictor space into $J$ distinct and non-overlapping regions
2. å¯¹è½äººåŒºåŸŸ $R_j$ çš„æ¯ä¸ªè§‚æµ‹å€¼ä½œåŒæ ·çš„é¢„æµ‹, é¢„æµ‹å€¼ä¸ºè®­ç»ƒé›†ä¸Šçš„å¹³å‡å€¼

æ„å»ºçš„æ­¥éª¤ï¼š

åˆ’åˆ†åŒºåŸŸä½¿å¾—æ•´ä½“æ®‹å·®RSSæœ€å°ï¼Œå³
$$
RSS(Tree)=\sum_{j=1}^{J} \sum_{i \in R_j} \left( y_i - \hat{y}_{R_j} \right)^2
$$
ä¸€ç§å¯è¡Œçš„æ–¹æ³•æ˜¯ï¼š**recursive binary splitting** é€’å½’äºŒå‰åˆ†è£‚ï¼š

Fine a $s$ , that splitting the predictor space $X_j$ into the regions $R_1(j,s)=\{X|X_j < s\}$ and $R_2(j,s)=\{X|X_j \geq s\}$ and minimize the equation
$$
\sum_{i: x_i \in R_1(j, s)} \left( y_i - \hat{y}_{R_1} \right)^2Â 
+Â 
\sum_{i: x_i \in R_2(j, s)} \left( y_i - \hat{y}_{R_2} \right)^2
$$
where
- $\hat{y}_{R_1}/\hat{y}_{R_2}$ is the mean response for the training observations


![](./SDSC5001.assets/file-20241205034533600.png)
- å·¦ä¸Šï¼šäºŒç»´ç‰¹å¾ç©ºé—´çš„åˆ’åˆ†ï¼Œ**ä¸èƒ½ç”±recursive binary splittingäº§ç”Ÿ**
- å³ä¸Šï¼šäºŒç»´ç¤ºä¾‹ä¸Šçš„é€’å½’äºŒè¿›åˆ¶åˆ†è£‚çš„è¾“å‡º
- å·¦ä¸‹ï¼šä¸å³ä¸Šè§’é¢æ¿ä¸­çš„åˆ†åŒºç›¸å¯¹åº”çš„æ ‘
- å³ä¸‹ï¼šä¸è¯¥æ ‘å¯¹åº”çš„é¢„æµ‹è¡¨é¢çš„é€è§†å›¾

Terminology
- terminal nodes ç»ˆç«¯èŠ‚ç‚¹
- tree is upside down, leaves are at the bottom of the tree.
- internal nodes å†…éƒ¨èŠ‚ç‚¹


**Decision trees** is Nonparametric éå‚æ•°æ¨¡å‹ , which models do not assume a specific functional form or distribution for the data. è¿™ç§æ¨¡å‹ä¸å‡å®šæ•°æ®æœ‰ç‰¹å®šçš„å‡½æ•°å½¢å¼æˆ–åˆ†å¸ƒ

#### Pruning

Using nonnegative tuning parameter $\alpha$, and make 
$$
C_\alpha(T) = \text{RSS}(T) + \alpha \cdot |T|
$$
- $|T|$ indicates the **number of terminal nodes** of the tree T
- Use K-fold cross-validation to choose Î±

ç®—æ³•æ­¥éª¤ï¼š
- é€æ­¥ä»æ ‘ä¸­åˆ é™¤å¶èŠ‚ç‚¹ï¼ˆæˆ–å­æ ‘ï¼‰ï¼Œæ¯æ¬¡åˆ é™¤åé‡æ–°è®¡ç®— $C_\alpha(T)$
- å‰ªææ—¶ï¼Œé€‰æ‹© **æœ€å°åŒ–å¢é‡ RSS çš„åˆ†æ”¯**ï¼Œå³ç§»é™¤å¯¹ $C_\alpha(T)$ å½±å“æœ€å°çš„åˆ†æ”¯
- ä½¿ç”¨ k-fold CV ç¡®å®šæœ€ä½³çš„ $\alpha$


### Classification Tree

Very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.

For a classification tree, we predict that each observation belongs to the **most commonly occurring class** of training observations in the region to which it belongs.

RSS cannot be used as a criterion for making the binary splits. We use **classification error rate** 
$$
E = 1 - \max_k (\hat{p}_{mk})
$$
where
- $\hat{p}_{mk}$ : åŒºåŸŸ $R_m$ ä¸­ï¼Œç±»åˆ« $k$ å åŒºåŸŸçš„æ¯”ä¾‹

example: åŒºåŸŸ $R_m$ ä¸­:

| ç±»åˆ«ï¼ˆ$k$ï¼‰ | æ ·æœ¬æ•°é‡ | æ¯”ä¾‹ ($\hat{p}_{mk}$â€‹) |
| ------- | ---- | -------------------- |
| $k=1$   | 30   | $\hat{p}_{m1} = 0.6$ |
| $k=2$   | 15   | $\hat{p}_{m2} = 0.3$ |
| $k=3$   | 5    | $\hat{p}_{m3} = 0.1$ |

#### Gini index

Why Gini index: **classification error rate** is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.
- å®ƒä»…å…³æ³¨ **å æ¯”æœ€å¤§çš„ç±»åˆ«**ï¼Œå¿½ç•¥äº†å…¶ä»–ç±»åˆ«çš„åˆ†å¸ƒæƒ…å†µ
	- $[0.6,0.3,0.1]$ ä¸ $[0.6,0.2,0.2]$ ä¸­çš„ $E$ ç›¸åŒ
- å› æ­¤ï¼Œåˆ†ç±»è¯¯å·®ç‡æ— æ³•åŒºåˆ†è¿™äº›æ›´ç»†å¾®çš„åˆ†å¸ƒå·®å¼‚

$$
G = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk}) = 1 - \sum_{k=1}^{K} \hat{p}_{mk}^2
$$

Gini index is referred to as a measure of **purity çº¯åº¦**

#### Cross-entropy
  
$$
H = -\sum_{k=1}^{K} \hat{p}_{mk} \log_2 (\hat{p}_{mk})
$$

![](./SDSC5001.assets/file-20241205140330377.png)

#### Notations

- Node: $t$ or $m$
- left child node: $t_l$
- The collection of all the nodes: $T$
- The collection of all the leaf nodes: $\tilde{T}$
- A split: $s$
- the set of splits: $S$


#### Trees Vs. Linear Models

- æ‹¥æœ‰çº¿æ€§ç»“æ„ï¼ŒLinear Modelsæ•ˆæœå¥½
- é«˜åº¦éçº¿æ€§ï¼ŒTreesæ•ˆæœå¥½
- Treesä¹Ÿæœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§å’Œå¯è§†åŒ–æ•ˆæœ
- å®è·µä¸­éœ€è¦ä½¿ç”¨ K-fold CV è¯„ä¼°

![](./SDSC5001.assets/file-20241205141846346.png)
- å·¦ä¸Šï¼šçº¿æ€§åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨Linear Modelsæ•ˆæœå¥½
- å³ä¸Šï¼šçº¿æ€§åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨Treesçš„æ•ˆæœè¾ƒå·®
- å·¦ä¸‹ï¼šéçº¿æ€§åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨Linear Modelsæ•ˆæœå·®
- å³ä¸‹ï¼šçº¿æ€§åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨Treesçš„æ•ˆæœè¾ƒå¥½

#### Advantages and Disadvantages of Trees

Advantages
- Trees are very easy to explain to people.
- Trees can be displayed graphically
- Trees can easily handle qualitativeå®šæ€§ predictors without the need to create dummy variables.
Disadvantages
- trees *do not have the same level of predictive accuracy* as some of the other regression and classification approaches
- non-robust

### Ensemble method

#### Bagging

åœ¨æ ‘æ¨¡å‹ï¼ˆå¦‚å†³ç­–æ ‘ï¼‰ä¸­ï¼Œåˆ†æ”¯è¶Šå¤šï¼ˆå³æ ‘çš„æ·±åº¦è¶Šå¤§ï¼‰ï¼Œæ¨¡å‹çš„å¤æ‚åº¦è¶Šé«˜ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹çš„**æ–¹å·®**å¢åŠ 

> **æ–¹å·®ï¼ˆVarianceï¼‰**ï¼š
> è¡¨ç¤ºæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„æ•æ„Ÿç¨‹åº¦
> $\text{Var}[\hat{f}(x)] = \mathbb{E}[(\hat{f}(x) - \mathbb{E}[\hat{f}(x)])^2].$

ä¸BootStrapçš„æ€è·¯ç›¸ä¼¼ï¼šgiven a set of n independent observations $Z_1, . . . , Z_n$, each with variance $\sigma^2$, the variance of mean of $\bar{Z}$  is $\sigma^2/n$. In other words, **averaging a set of observations reduces variance.** **æ±‚å¹³å‡å¯ä»¥å‡å°æ–¹å·®**

å› æ­¤ï¼Œè¦å‡å°æŸç§ç»Ÿè®¡å­¦ä¹ æ–¹æ³•çš„æ–¹å·®ä»è€Œå¢åŠ é¢„æµ‹å‡†ç¡®æ€§ï¼Œæœ‰ä¸€ç§å¾ˆè‡ªç„¶çš„æ–¹æ³•ï¼šä»æ€»ä½“ä¸­æŠ½å–å¤šä¸ªè®­ç»ƒé›†ï¼ˆå®é™…ä¸Šä¸å¯è¡Œï¼Œéœ€è¦ä½¿ç”¨BootStrapï¼‰ï¼Œå¯¹æ¯ä¸ªè®­ç»ƒé›†åˆ†åˆ«å»ºç«‹é¢„æµ‹æ¨¡å‹ï¼Œå†å¯¹ç”±æ­¤å¾—åˆ°çš„å¤šä¸ªé¢„æµ‹å€¼æ±‚å¹³å‡
$$
\hat{f}_{\text{avg}}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{b}(x).
$$
å¯¹äºqualitativeå®šæ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨majority vote: the overall prediction is the most commonly occurring class among the B predictions. 
B ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„ç±»ä½œä¸ºé¢„æµ‹å€¼

å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š
1. åˆ©ç”¨BootStrapåˆ›å»ºåˆå§‹è®­ç»ƒé›†çš„å¤šä¸ªå‰¯æœ¬
2. è®­ç»ƒå¤šä¸ªåŸºæ¨¡å‹
3. é›†æˆé¢„æµ‹
	1. **å›å½’ä»»åŠ¡**ï¼šå–åŸºæ¨¡å‹é¢„æµ‹çš„å¹³å‡å€¼
	2. **åˆ†ç±»ä»»åŠ¡**ï¼šé‡‡ç”¨æŠ•ç¥¨æ³•ï¼Œé€‰æ‹©é¢„æµ‹æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœ

#### Out-of-Bag Error Estimation

å¯¹äºBootStrapä¸­ï¼Œå¯ä»¥è¯æ˜
- **çº¦ 63.2%** çš„æ ·æœ¬è¢«æŠ½ä¸­ï¼ˆåœ¨è®­ç»ƒè¿™æ£µæ ‘æ—¶ä½¿ç”¨ï¼‰
- **å‰©ä¸‹çš„çº¦ 36.8%** çš„æ ·æœ¬æ²¡æœ‰è¢«æŠ½ä¸­ï¼ˆç§°ä¸ºâ€œè¢‹å¤–æ ·æœ¬â€æˆ– Out-of-Bag æ ·æœ¬ï¼‰

å¯¹äºæ¯ä¸€æ£µæ ‘ï¼ŒOOB æ ·æœ¬å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªâ€œ**éªŒè¯é›†**â€ï¼Œå¯¹æ‰€æœ‰æ ·æœ¬ï¼Œè®¡ç®— OOB æ ·æœ¬çš„é¢„æµ‹è¯¯å·®

$$
\text{OOB Error} = \frac{1}{n} \sum_{i=1}^n \mathbb{1}(\hat{y}_i^{\text{OOB}} \neq y_i)
$$
- $\hat{y}_i^{\text{OOB}}$Â  æ˜¯Â  x_iÂ  åœ¨ OOB æ ·æœ¬ä¸­çš„é¢„æµ‹ç»“æœï¼ˆé€šè¿‡æœªä½¿ç”¨Â  x_iÂ  çš„æ ‘å¾—åˆ°ï¼‰
- $y_i$Â  æ˜¯æ ·æœ¬çš„çœŸå®æ ‡ç­¾

OOB error is virtually **equivalent to** leave-one-out cross-validation error.

#### Variable Importance Measures

Methodï¼š
- In the case of **bagging regression trees**: Record the total amount that the *RSS* is **decreased** due to splits over a given predictor, averaged over all B trees.
- In the case of **bagging classification trees**: Add up the total amount that the *Gini index* is **decreased** by splits over a given predictor, averaged over all B trees.

#### Random Forests

éšæœºæ£®æ— (random forest) é€šè¿‡å¯¹æ ‘ä½œ*å»ç›¸å…³å¤„ç†*,å®ç°äº†å¯¹è£…è¢‹æ³•æ ‘çš„æ”¹é€ 

å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š
1. éšæœºæ£®æ—ä¸ºæ¯æ£µæ ‘æ„å»ºä¸€ä¸ªå­æ ·æœ¬ï¼ˆé€šè¿‡ Bootstrap æŠ½æ ·ï¼‰ã€‚
2. åœ¨æ„å»ºè¿™æ£µæ ‘çš„æ¯ä¸€ä¸ªåˆ†è£‚èŠ‚ç‚¹æ—¶ï¼Œä»å…¨éƒ¨Â $p$Â ä¸ªç‰¹å¾ä¸­éšæœºé€‰æ‹©Â $m$ ä¸ªç‰¹å¾ã€‚
3. ä»…åœ¨è¿™Â $m$ ä¸ªç‰¹å¾ä¸­ï¼Œæ‰¾åˆ°ä¸€ä¸ªç”¨äºå½“å‰èŠ‚ç‚¹åˆ†è£‚çš„æœ€ä½³ç‰¹å¾åŠå…¶åˆ†è£‚ç‚¹ã€‚

Why:
- å¦‚æœæ²¡æœ‰è¿™ç§éšæœºç‰¹å¾é€‰æ‹©ï¼Œæ¯æ£µæ ‘éƒ½å¯èƒ½è¿‡äºä¾èµ–å¼ºç‰¹å¾ï¼Œå¯¼è‡´æ‰€æœ‰æ ‘çš„ç»“æ„ç›¸ä¼¼
- å¦‚æœÂ $m$ è¾ƒå°ï¼Œæ¯æ£µæ ‘çš„åˆ†è£‚æ›´éšæœºï¼Œæ ‘çš„å¤šæ ·æ€§æ›´å¤§ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ›´å¼º

#### Boosting

Boosting çš„æ ¸å¿ƒæ€æƒ³æ˜¯**é¡ºåºè®­ç»ƒå¤šä¸ªå¼±æ¨¡å‹**ï¼Œæ¯ä¸ªæ¨¡å‹çš„é‡ç‚¹æ˜¯ä¿®æ­£å‰ä¸€ä¸ªæ¨¡å‹çš„é”™è¯¯é¢„æµ‹

Like bagging, boosting involves combining a large number of decision trees, $f^1, . . . , f^B$,  ä¸baggingä¸åŒçš„æ˜¯ï¼ŒÂ Boosting ä¸ä½¿ç”¨ Bootstrapï¼Œè€Œæ˜¯é€šè¿‡è¿­ä»£ä¾æ¬¡ç”Ÿæˆæ ‘$f^1, . . . , f^B$

Gradient Boosting è®­ç»ƒæ€æƒ³ï¼š
1. åˆå§‹åŒ–æ¨¡å‹
2. è¿­ä»£
	1. åˆ©ç”¨å½“å‰æ®‹å·®æ‹Ÿåˆæ ‘
	2. å°†æ–°æ ‘åŠ å…¥æ¨¡å‹
	3. æ›´æ–°æ•´ä½“æ®‹å·®
Why works? Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard å¯¹æ•°æ®çš„ä¸¥æ ¼å¥‘åˆ and potentially overfittingè¿‡æ‹Ÿåˆ, the boosting approach learns slowlyèˆ’ç¼“.

AdaBoost æ˜¯é€šè¿‡è°ƒæ•´**æƒé‡**æ¥è°ƒæ•´æ¨¡å‹ï¼Œè€Œä¸æ˜¯æ®‹å·®
AdaBoost æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒé€šè¿‡è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œ**è®©åç»­çš„å¼±å­¦ä¹ å™¨æ›´åŠ å…³æ³¨ä¹‹å‰çš„é”™è¯¯æ ·æœ¬**ï¼Œä»è€Œé€æ­¥æ”¹è¿›æ•´ä½“æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½
**AdaBoost ä¼ªä»£ç **ï¼š
- è®­ç»ƒé›† $D = \{(x_1, y_1), \dots, (x_n, y_n)\}$
- åŸºå­¦ä¹ å™¨ $f(x)$
- æœ€å¤§è¿­ä»£æ¬¡æ•° $M$
Algorithm:
1. åˆå§‹åŒ–æ ·æœ¬æƒé‡ $w_i = \frac{1}{n}, \forall i = 1, \dots, n$
2. For $m = 1$ to $M$:
	1. ä½¿ç”¨å½“å‰æ ·æœ¬æƒé‡ $w_i$ï¼Œè®­ç»ƒåŸºå­¦ä¹ å™¨ $f_m(x)$
		- æ ·æœ¬æƒé‡è¶Šé«˜ï¼Œå…¶å¯¹åˆ†è£‚èŠ‚ç‚¹é€‰æ‹©çš„å½±å“è¶Šå¤§
	2. è®¡ç®—åŸºå­¦ä¹ å™¨çš„åŠ æƒé”™è¯¯ç‡ï¼š$\varepsilon_m = \frac{\sum_{i=1}^n w_i \cdot \mathbb{1}(f_m(x_i) \neq y_i)}{\sum_{i=1}^n w_i}$
		- é”™è¯¯åˆ†ç±»çš„é«˜æƒé‡æ ·æœ¬å¯¹æŸå¤±å‡½æ•°çš„è´¡çŒ®æ›´å¤§ï¼ŒåŸºå­¦ä¹ å™¨ä¼šä¼˜å…ˆä¼˜åŒ–è¿™äº›æ ·æœ¬
	3. è®¡ç®—åŸºå­¦ä¹ å™¨çš„æƒé‡ï¼š$\alpha_m = \frac{1}{2} \ln\left(\frac{1 - \varepsilon_m}{\varepsilon_m}\right)$
	4. **æ›´æ–°æ ·æœ¬æƒé‡**ï¼š$w_i \leftarrow w_i \cdot \exp\left(-\alpha_m y_i f_m(x_i)\right), \quad \forall i$
		- å…¶ä¸­Â  $\alpha_m$Â  æ˜¯å½“å‰å¼±å­¦ä¹ å™¨çš„æƒé‡ã€‚
		- å¦‚æœÂ  $f_m(x_i) = y_i$ ï¼ˆæ­£ç¡®åˆ†ç±»ï¼‰ï¼Œåˆ™Â  $w_i$Â  ä¼šå‡å°ã€‚
		- å¦‚æœÂ $f_m(x_i) \neq y_i$ï¼ˆé”™è¯¯åˆ†ç±»ï¼‰ï¼Œåˆ™Â $w_i$ ä¼šå¢å¤§
3. è¾“å‡ºæœ€ç»ˆæ¨¡å‹ï¼š$F(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m f_m(x)\right)$
	- æ¯æ£µå­æ ‘ï¼ˆå¼±å­¦ä¹ å™¨ï¼‰çš„**æƒé‡**$\alpha_m$å¿…é¡»è®°å½•ï¼Œç”¨äºé¢„æµ‹
	- æ ·æœ¬æƒé‡ $w_i$ æ˜¯ä¸è®°å½•çš„ï¼Œåªç”¨äºæŒ‡å¯¼è®­ç»ƒé˜¶æ®µ

## Support Vector Machines

### Optimization 

åŸé—®é¢˜ï¼ˆPrimary Problemï¼‰
- æœ€å°åŒ–: $f(w)$
- é™åˆ¶æ¡ä»¶: 
	- $g_i(w)\leq 0\quad \forall i = 1, \dots, N$
	- $h_i(w) = 0\quad \forall i = 1, \dots, N$
åŸé—®é¢˜çš„å®šä¹‰æ˜¯éå¸¸æ™®ä¸–çš„å®šä¹‰ï¼Œå³
- æœ€å°åŒ– å¯ä»¥ç­‰ä»·äº æœ€å¤§åŒ–
- $g_i(w)\leq 0$ ç­‰ä»·äº $g_i(w)\geq 0$
- $h_i(w) = 0$ ç­‰ä»·äº $h_i(w) = C$

å¯¹å¶é—®é¢˜(Dual Problem)

1. å®šä¹‰ï¼š
$$
\begin{align}
&L(w,\alpha,\beta)\\
&=f(w)+\alpha^Tg(w)+\beta^T h(w)
\end{align}
$$
2. å¯¹å¶é—®é¢˜çš„å®šä¹‰ï¼š
$$
\text{maximum }[\theta(\alpha,\beta)=inf_{\text{for all }w}\left(L(w,\alpha,\beta)\right)], \quad \text{subject to: } \alpha \geq 0
$$
- $inf_{\text{for all }w}\left(L(w,\alpha,\beta)\right)$ æ„æ€æ˜¯ï¼šéå†æ‰€æœ‰çš„$w$, å¹¶å›ºå®š$\alpha,\beta$åï¼Œæ±‚$L$ çš„æœ€å°å€¼
- å†…éƒ¨æœ€å°ï¼Œå¤–éƒ¨æœ€å¤§

å¼ºå¯¹å¶å®šç†ï¼š
Â - f(w)Â  æ˜¯ä¸€ä¸ª **å‡¸å‡½æ•°**ï¼›
- çº¦æŸæ¡ä»¶Â  $g(w) = Aw + b$Â  æ˜¯ **çº¿æ€§ç­‰å¼çº¦æŸ**ï¼›
- çº¦æŸæ¡ä»¶Â  $h(w) = Cw + d$Â  æ˜¯ **çº¿æ€§ä¸ç­‰å¼çº¦æŸ**ï¼Œ
å‡è®¾ $w^*$Â æ˜¯åŸé—®é¢˜çš„æœ€ä¼˜è§£ï¼Œ$\alpha^*, \beta^*$æ˜¯å¯¹å¶é—®é¢˜çš„è§£ï¼Œåˆ™
$$
f(w^*) = \theta(\alpha^*, \beta^*),
$$


### Maximal Margin Classifier

#### Hyperplane

In a $p$-dimensional space, a *hyperplane è¶…å¹³é¢* is a flat affine subspace å¹³é¢ä»¿å°„å­ç©ºé—´ of dimension $p âˆ’ 1$.
- äºŒç»´ç©ºé—´ä¸­çš„è¶…å¹³é¢æ˜¯ä¸€æ¡ç›´çº¿
- ä¸‰ç»´ç©ºé—´çš„è¶…å¹³é¢æ˜¯ä¸€ä¸ªé¢

p-dimensional space's *hyperplane* is defined by the equationï¼š
$$
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0
$$


![](./SDSC5001.assets/file-20241206222932258.png)
- **margin** (ç®­å¤´): distance from the observations to the hyperplane
- **maximal margin classifier/optimal separating hyperplane** (é»‘çº¿): maximal margin hyperplane

If $Î²0, Î²1, . . . , Î²_p$ are the coefficients of the **maximal margin hyperplane**, then the maximal margin classifier classifies the test observation $x^âˆ—$ based on the sign of $f(x^âˆ—) = Î²_0 + Î²_1x_1^âˆ— + Î²_2x_2^âˆ— + Â· Â· Â· + Î²_px^âˆ—_p$

#### Construction of the Maximal Margin Classifier

Given a dataset ofÂ  nÂ  training observations:
$$
\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}, \quad x_i \in \mathbb{R}^p, \quad y_i \in \{-1, 1\}
$$
Maximal Margin Classifier is a Linear Model:
$$
w^TX+b=0
$$
$$
w =
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_n
\end{bmatrix}\\
\quad
X =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
w_n
\end{bmatrix}
$$
We have to parameter W and X

to make sure separable ä¿è¯çº¿æ€§å¯åˆ†:

$$
y_i ( w^TX+b ) \geq 1, \quad \forall i = M, \dots, n
$$
- $y_i=1$ $\to$ $W^TX+b\geq 0$ ä»£è¡¨çœŸä¾‹åœ¨è¶…å¹³é¢å€¼åŸŸå¤§äº0çš„éƒ¨åˆ†
- $y_i=-1$ $\to$ $W^TX+b<0$ ä»£è¡¨è´Ÿä¾‹åœ¨è¶…å¹³é¢å€¼åŸŸå°äº0çš„éƒ¨åˆ†
- æ»¡è¶³ä¸Šå¼çš„æ¡ä»¶å³çº¿æ€§å¯åˆ†

The distant of margin from $x_i$:
$$
d=\frac{|w^Tx_0+b|}{||w||}
$$
To maximize $d$:
$$
\min_w \frac{1}{2} \|w\|^2
$$
åœ¨ SVM ä¸­ï¼Œä¸ºäº†ç®€åŒ–ä¼˜åŒ–é—®é¢˜ï¼Œçº¦æŸè¢«æ ‡å‡†åŒ–ä¸ºï¼š
$$
y_i \left(w \cdot x_i + b\right) \geq 1 \quad \forall i.
$$
è¿™ä¸ªçº¦æŸå›ºå®šäº†Â $w$ çš„å°ºåº¦ï¼Œé—´æ¥æ¶ˆé™¤äº† $w$ å’ŒÂ $b$ çš„ç¼©æ”¾è‡ªç”±åº¦ã€‚å› æ­¤ï¼Œä¼˜åŒ–é—®é¢˜åªéœ€è¦å…³æ³¨ $||w||$ çš„å¤§å°, ä¸ºäº†æ±‚å¯¼æ–¹ä¾¿ï¼ŒåŠ ä¸Š$\frac{1}{2}$

ç»¼ä¸Šï¼ŒSVMæ˜¯ä¸€ä¸ªäºŒæ¬¡è§„åˆ’é—®é¢˜ï¼ˆå‡¸ä¼˜åŒ–é—®é¢˜ï¼‰ï¼š
$$
\min_w \frac{1}{2} \|w\|^2, \quad \text{subject to: } y_i \left( w \cdot x_i + b \right) \geq 1, \quad \forall i
$$
å¯¹äºè¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå”¯ä¸€çš„è§£ï¼ˆçº¿æ€§å¯åˆ†æ—¶ï¼‰ï¼Œæˆ–è€…è§£ä¸å­˜åœ¨ï¼ˆçº¿æ€§ä¸å¯åˆ†æ—¶ï¼‰

### Support Vector Classifiers

Maximal Margin Classifier å¯¹äºå¯¹å•ä¸ªè§‚æµ‹å€¼çš„å˜åŒ–æå…¶æ•æ„Ÿï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ

we might be willing to consider a classifier based on a hyperplane that **does not perfectly** separate the two classes

**ä¼˜åŒ–ç›®æ ‡**
$$
\min \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i
$$
-  $w$: å†³ç­–è¶…å¹³é¢çš„æ³•å‘é‡çš„èŒƒæ•°ï¼Œè¡¨ç¤ºé—´éš”å¤§å°ï¼ˆé—´æ¥ä¼˜åŒ–é—´éš”ï¼‰
- $C$: æƒ©ç½šç³»æ•°ï¼Œå¹³è¡¡é—´éš”æœ€å¤§åŒ–å’Œå¯¹åˆ†ç±»é”™è¯¯æ ·æœ¬çš„å®¹å¿
- $\xi_i$: æ¾å¼›å˜é‡ï¼ˆSlack Variableï¼‰ï¼Œå…è®¸ä¸€äº›æ ·æœ¬è¿åé—´éš”çº¦æŸ
**çº¦æŸæ¡ä»¶**ï¼ˆSubject toï¼‰
1. $y_i \left( w^T x_i + b \right) \geq 1 - \xi_i, \quad \forall i = 1, \dots, N$
2. $\xi_i \geq 0, \quad \forall i = 1, \dots, N$

æ³¨æ„ï¼š
- $\xi_i$ ä¹Ÿæ˜¯éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†
- $C$ æ˜¯è‡ªå·±é…ç½®çš„è¶…å‚æ•°

$C \sum_{i=1}^N \xi_i$ å¯ä»¥çœ‹åšæ­£åˆ™é¡¹ï¼Œä¿è¯ä¸€äº›çº¿æ€§ä¸å¯åˆ†çš„å€¼å˜æˆå¯åˆ†çš„

### Support Vector Machines


![](./SDSC5001.assets/file-20241207034345048.png)
Left: The observations fall into two classes, with a **non-linear boundary** between them. Right: The *support vector classifier* seeks a linear boundary, and consequently **performs very poorly.**

SVMé€šè¿‡å°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºé«˜ç»´æ•°æ®ï¼Œå¹¶æŸ¥æ‰¾å…¶é«˜ç»´æ•°æ®ä¸­çš„çº¿æ€§*support vector classifier*ï¼Œè€Œä¸ç ´åçº¿æ€§æ¡ä»¶
$$
X \to \phi(X)
$$
---
Q: ä¸ºä»€ä¹ˆå‡ç»´å¯ä»¥ä½¿å¾—ä½ç»´non-linearæ•°æ®çº¿æ€§å¯åˆ†ï¼Ÿ

ç»å…¸**XOR**é—®é¢˜ï¼š

- $C_1$ï¼ˆç±»åˆ« 1ï¼Œè¾“å‡ºä¸º 1ï¼‰ï¼š(0, 1), (1, 0)
- $C_2$ï¼ˆç±»åˆ« 2ï¼Œè¾“å‡ºä¸º 0ï¼‰ï¼š(0, 0), (1, 1)

$$
\phi(x) = \begin{bmatrix} a \\ b \end{bmatrix} \longrightarrow \phi(x) = \begin{bmatrix} a^2 \\ b^2 \\ ab \end{bmatrix}.
$$

å°†æ•°æ®ç‚¹æ˜ å°„åˆ°ä¸‰ç»´ç©ºé—´åï¼š

| åŸå§‹ç‚¹$(a,b)$ | æ–°ç‰¹å¾$(a^2,b^2,ab)$ | ç±»åˆ«    |
| ---------- | ----------------- | ----- |
| (0,0)      | (0,0,0)           | $C2$â€‹ |
| (0,1)      | (0,1,0)           | $C1$â€‹ |
| (1,0)      | (1,0,0)           | $C1$â€‹ |
| (1,1)      | (1,1,1)           | $C2$â€‹ |
åœ¨æ–°ç‰¹å¾ç©ºé—´ä¸­ $C_1$ å¯ä»¥è¢«ä¸€ä¸ª**å¹³é¢**ä¸Â  $C_2$Â åˆ†å¼€

---
Q: å¦‚ä½•é€‰å–$\phi(X)$

å¯ä»¥è¯æ˜ï¼š*å½“$X$å‡ç»´è‡³æ— é™å¤§ï¼Œä»»ä½•éšæœºçš„æ•°æ®éƒ½çº¿æ€§å¯åˆ†*ï¼›ä½†æ˜¯å°è¯•çº¿æ€§åˆ†å‰²æ—¶ï¼Œä¼šå¯¼è‡´$W$çš„ç»´åº¦ä¹Ÿæ— é™å¤§ï¼Œå®é™…ä¸å¯è¡Œ

è§£å†³ï¼šå¯ä»¥è¯æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ä¸çŸ¥é“**æ— é™ç»´**$\phi(X)$çš„å…·ä½“è¡¨è¾¾ï¼Œåªéœ€è¦çŸ¥é“ä¸€ä¸ªæ ¸å‡½æ•°(**Kernel Function**)
$$
K(X_1,X_2)=\phi(X_1)^T\phi(X_2)
$$
åˆ™ä¼˜åŒ–ç›®æ ‡
$$
\begin{align}
&\min \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i 
\\
&\text{subject to }\\
&\quad \text{1. } y_i \left( w^T \phi(x_i) + b \right) \geq 1 - \xi_i, \quad \forall i = 1, \dots, N
\\
&\quad \text{2. }\xi_i \geq 0, \quad \forall i = 1, \dots, N
\end{align}
$$
ä¾ç„¶çº¿æ€§å¯åˆ†

å¸¸è§çš„Kernel Function
$$
K(X_1,X_2)=\exp(-\frac{||X_1-X_2||^2}{2\sigma^2})=\phi(X_1)^T\phi(X_2)
$$

è¡¥å……æ¡ä»¶ï¼š
$K(X_1,X_2)=\phi(X_1)^T\phi(X_2)$çš„å……è¦æ¡ä»¶ï¼š
- äº¤æ¢æ€§ $K(X_1, X_2) = K(X_2, X_1),$
- **Mercerâ€™s Theorem**
	- åŠæ­£å®šå‹ $\forall \mathbf{a} \in \mathbb{R}^n, \; \mathbf{a}^T K \mathbf{a} \geq 0,$
	- Â $K(X_1, X_2)$Â  åœ¨æ ·æœ¬ç©ºé—´Â  $\mathcal{X} \times \mathcal{X}$Â  ä¸Šè¿ç»­ï¼›
	- å¯¹äºä»»æ„å¹³æ–¹å¯ç§¯å‡½æ•°Â $g(X)$ ï¼š
		- $\int_{\mathcal{X}} \int_{\mathcal{X}} g(X_1) K(X_1, X_2) g(X_2) \, dX_1 \, dX_2 \geq 0.$
---
Q: å¦‚ä½•åˆ©ç”¨Kernel Functionè§£SVMé—®é¢˜

åˆ©ç”¨å¼ºå¯¹å¶å®šç†æ±‚SVMä¼˜åŒ–é—®é¢˜çš„ç­‰ä»·å¯¹å¶é—®é¢˜ï¼Œæ­¤æ—¶å¯¹å¶é—®é¢˜å¯ä»¥å¾—åˆ°æ¶ˆå»$\phi(X_1)^T\phi(X_2)$è€Œå¾—åˆ°$K$çš„å¼å­

SVM ç®—æ³•ï¼š
1. è®­ç»ƒæµç¨‹
	- è¾“å…¥ï¼š
$$
\{(X_i, y_i)\}_{i=1}^N
$$
2. è§£ä¼˜åŒ–é—®é¢˜
	- æœ€å¤§åŒ–ï¼š
$$
\Theta(a) = \sum_{i=1}^N a_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j K(X_i, X_j)
$$
	- çº¦æŸæ¡ä»¶ï¼š
$$
\begin{aligned}

1. & \quad 0 \leq a_i \leq C, \quad \forall i = 1, \dots, N, \\

2. & \quad \sum_{i=1}^N a_i y_i = 0.

\end{aligned}
$$
3. è®¡ç®—$b$
	- æ‰¾åˆ°ä¸€ä¸ª $0 < a_i < C$ï¼Œç„¶åè®¡ç®—ï¼š
$$
b = \frac{1 - y_i \sum_{j=1}^N a_j y_j K(X_i, X_j)}{y_i}.
$$
SVM æµ‹è¯•æµç¨‹ï¼š
$$
\begin{cases}Â 
\text{If } \sum_{i=1}^N a_i y_i K(X_i, X) + b \geq 0, & \text{then } y = +1, \\
\text{If } \sum_{i=1}^N a_i y_i K(X_i, X) + b < 0, & \text{then } y = -1.
\end{cases}
$$
#### SVMs with More than Two Classes

SVM äº‹å®ä¸Šæ˜¯é’ˆå¯¹äºŒåˆ†ç±»åˆ¶å®šçš„æ¨¡å‹ï¼Œå¤šå˜é‡ä¸Šçš„æ•ˆæœå¹¶ä¸æ˜¯å¾ˆå¥½

- One-Versus-One Classification
	1. AÂ  vsÂ  BÂ 
	2. AÂ  vsÂ  CÂ 
	3. BÂ  vsÂ  C
- One-Versus-All Classification
	1. AÂ  vs B C
	2. BÂ  vs A C
	3. CÂ  vs A B

#### Relationship to Logistic Regression

- é€»è¾‘å›å½’ä¸SVMæ¯”è¾ƒç›¸ä¼¼
- SVMä¸ä¼šå—åˆ°æ­£ç¡®åˆ†ç±»é—´éš”ä¸­çš„ç¦»ç¾¤ç‚¹çš„å½±å“ï¼Œåªå—åˆ°åˆ†ç±»é—´éš”é™„è¿‘çš„ç‚¹çš„å½±å“ï¼›è€Œé€»è¾‘å›å½’ä¼šå¯¹è®¡ç®—åˆ†ç±»æ­£ç¡®éƒ¨åˆ†ä¾ç„¶è®¡ç®—æŸå¤±