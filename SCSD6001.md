# SCSD6001

## Lecture1

时间序列分析的目标：描述、预测、控制和假设检验

### 时序中的随机变量

- 随机变量: 随机变量 $X$ 是一个函数，它将一个样本空间中的事件映射到实数（或其他数值）。其值是随机的，即它的取值受某个概率分布的控制
- 在时间序列分析中，**随机变量 $X_t$** 是指时间序列的每一个观察值，**它在每个时间点上是随机的，并且其值遵循某个概率分布**


### Stochastic process 随机过程

- ${X_t}$ 按照时间索引的过程就是随机过程
- 是一种建模方法，研究不同时间点的关系
- The observed values of a stochastic process are referred to as a _realization_ of the stochastic process. 也就是说，我们在某个时间序列中看到的数据只是随机过程的一个实现realization
- An _observed time series_ is considered to be _one realization_ of a stochastic process. 观测到的时间序列是随机过程的一种实现

> 什么叫“随机过程”？这个随机和random有关系吗？
>
> GPT：
>
> “随机过程”中的“随机”确实与“random”（随机）有关。它指的是一个随时间演变的系统或现象，其中的结果不是确定的，而是随机的。每个时刻的数据点（或状态）是由一组随机变量表示的。
> 
> 具体来说，“随机过程”是指一个按时间顺序排列的随机变量集合，表示某个系统或现象在时间上的变化。由于这些随机变量的结果是不可预知的，所以我们说它是“随机”的。这与“random”一词的含义一致，指的是结果或状态是通过某种概率分布产生的，而不是确定的。
> 
> 例如，股票价格、天气变化、人口增长等都可以用随机过程来描述。它们在不同时间点的值是由多个因素影响的，并且这些值无法完全预测，因此被视为“随机”的。
> 
> 总结来说，随机过程是描述系统或现象如何在时间上以随机方式变化的数学模型，其中的“随机”与“random”密切相关，表示不确定性和概率性。

### 随机过程 vs. 随机变量：

在时间序列分析中，我们通常处理的是**随机过程$\{X_t\}$ (注意有大括号,是多个变量)**，**它是随时间变化的随机变量序列$X_t$ (注意这是一个变量,没有大括号)**

> 换言之: 多个会变化的**随机变量$X_t$**,并且变化是以时间为维度的,就变成了**随机过程$\{X_t\}$**

假设有一个随机过程$\{X_t\}$ ，每个时间点$t$上我们都观察到一个随机变量$X_t$ 

#### 相关符号

- $\{X_t\}$ 随机过程
- $X_t$ 在随机过程$t$中的一个**随机变量**
- $x_t$ 是一个随机过程$t$中的一个**观测值**

### White noise 白噪声

白噪声是一种随机信号，其中各个时间点上的值都是彼此不相关的，也就是这些随机变量之间没有相关性。

它的**自相关函数是零**，表示在不同时间点的信号是完全独立的。

$w_t \sim \text{wn}(0, \sigma_w^2)$

### Gaussian white noise 高斯白噪声

一种特殊的白噪声,保证噪声的分布满足正态分布

$w_t \sim \text{N}(0, \sigma_w^2)$

#### 相关符号
- $w_t$ 白噪声
- $\sigma_w^2$ 方差
- $wn$ 白噪声
- $N(0, \sigma_w^2)$ 正态分布


### Time series modeling 时间序列模型

时间序列模型的基本结构可以表示为：

$$
x_t = m_t + s_t + e_t
$$

其中：
- $x_t$ 表示时间序列的**观测值**；
> 注意是观测值, 这就是为什么是小x而不是大X
- $m_t$ 是**趋势成分**（trend component），表示时间序列的长期变化趋势；
- $s_t$ 是**季节性成分**（seasonal component），表示时间序列的周期性波动（如季节、月度变化等）；
- $e_t$ 是**残差**（residual），表示无法通过趋势或季节性成分解释的随机噪声或误差, 例如$w_t$。

这个模型将时间序列分解为三个部分，以便更好地理解和预测时间序列的行为。

> 请问噪声属于什么部分？
> 
> GPT:
> 
> 在时间序列模型中，**噪声**属于公式中的**残差部分**（residual），即 $e_t$ 。残差代表的是时间序列中无法通过趋势（$m_t$）和季节性成分（$s_t$）解释的部分。
>
> 具体来说：
> - **噪声（noise）** 是指时间序列中由于随机因素产生的波动或误差，通常被视为不可预测的随机变化。
> - **残差（residual）** 是实际观测值与模型预测值之间的差异，包括噪声成分。
>
> 因此，在时间序列模型 $x_t = m_t + s_t + e_t$ 中，**噪声就是 $e_t$**，它通常表示为服从某种概率分布的随机变量（如白噪声、高斯白噪声等），是建模过程中用来描述不规则波动的重要部分。


### 均值性质

- 均值具有线性性(线性性)

$E(aX+bY)=aE(X)+bE(Y)$

- 常数的期望即本身(常数性)

$E(c)=c$

- **独立随机变量和**的 _均值_ == **独立随机变量均值**的 _和_ (独立性)

$E\left( \frac{1}{n} \sum_{i=1}^{n} X_i \right) = \frac{1}{n} \sum_{i=1}^{n} E(X_i)$

### 均值与时间序列

#### Example1

$v_t=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1})
$

$E(t_t)?$

根据均值的线性性,可得:

$$
\begin{align}
E(v_t) &= \frac{1}{3} E(w_{t-1} + w_t + w_{t+1}) \\
       &= \frac{1}{3} E(w_{t-1}) + \frac{1}{3} E(w_t) + \frac{1}{3} E(w_{t+1}) \\
       &= 0
\end{align}
$$

**性质：⽆论多少个⽆关噪声求和，期望都是0**

#### Example2

$x_t = \delta t + \sum_{j=1}^{t} w_j$

$E(x_t)?$

**注意这里$t$是一个固定的时间索引,需要看成常数**

根据均值的线性性与常数型,可得

$$
\begin{align}
E(x_t) &= E\left( \delta t + \sum_{j=1}^{t} w_j \right) \\
       &= \delta t + \sum_{j=1}^{t} E(w_j) \\
       &= \delta t
\end{align}
$$

**性质:在具有线性趋势的时间序列模型中，期望值反映的是时间序列的趋势部分，而不受随机噪声的影响**

#### Example3

$x_t = A \cos(2 \pi \omega t + \phi) + w_t$

$E(x_t)?$

与上个Example一样,同样t也是看成常数

$$
\begin{align}
E(x_t) &= E\left( A \cos(2 \pi \omega t + \phi) + w_t \right) \\
&= A \cos(2 \pi \omega t + \phi) + E(w_t)\\
&= A \cos(2 \pi \omega t + \phi)
\end{align}
$$

**噪声不会影响时间序列的期望值**

### 协方差

#### 协方差的定义

协方差 $\text{Cov}(X, Y)$ 是一种用来衡量两个随机变量之间线性关系的统计量。它反映了两个变量如何一起变化。


$\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]$

- $\mathbb{E}[X]$ 和 $\mathbb{E}[Y]$ 分别是随机变量 $X$ 和 $Y$ 的期望值
- $(X - \mathbb{E}[X])$ 表示随机变量 $X$ 偏离其期望值的程度
- $(Y - \mathbb{E}[Y])$ 表示随机变量 $Y$ 偏离其期望值的程度
- 协方差实际上是对 $(X - \mathbb{E}[X])$ 和 $(Y - \mathbb{E}[Y])$ 这两个偏差的乘积的期望

#### 协方差的意义

- 如果协方差为正，意味着这两个变量趋向于同方向变化
- 如果协方差为负，意味着它们趋向于相反方向变化
- 如果协方差接近 0，意味着两个变量之间没有线性关系

#### 协方差的性质

1. 对称性

$\text{Cov}(X, Y) = \text{Cov}(Y, X)$

2. 退化方差

方差是协方差的特例。当 $X = Y$ 时，协方差就是随机变量的方差：

$\text{Cov}(X, X) = \text{Var}(X)$

3. 缩放不变性

假设 $a$ 和 $b$ 是常数，则：

$\text{Cov}(aX + b, Y) = a \cdot \text{Cov}(X, Y)$

这意味着，如果对一个随机变量进行线性变换，它的协方差会按比例缩放

4. 分配性质

对于三个随机变量 $X$、$Y$ 和 $Z$

$\text{Cov}(X + Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z)$

$\text{Cov}(X, Y + Z) = \text{Cov}(X, Y) + \text{Cov}(X, Z)$



### Autocovariance function 自协方差函数 $\gamma(s,t)$

用于衡量相关性

$\bold{\gamma(s,t)} = \text{Cov}(X_s, X_t) = E[(X_s - \mu_s)(X_t - \mu_t)]$ 

是用来衡量**同一随机过程**中不同**时间点**对应的**随机变量**之间的线性相关性

> 为什么时间点之间可以衡量协方差？衡量协方差不应该是利用随机变量，代表的是多个值吗？
>
>
> 虽然 $s$ 和 $t$ 表示的是时间点, 但是对于随机过程$\{X_t\}$来说, $s$ 和 $t$ 代表的是 $X_s$ 和 $X_t$ 随机变量, 但是实际上我们关心的是$X_s$ 和 $X_t$ 随机变量之间的协方差


- 当$\gamma(s,t) = 0$ 代表无相关性
- 当 $s=t$ ，协方差退化为方差

> 为什么当 $s=t$ ，协方差退化为方差?
>
> 从公式理解:
>
> $\mu_s = E[X_s]$
> 
> $Var(X_s)=E[(X_s-E[X_s])^2]=E[(X_s - \mu_s)(X_s - \mu_s)=Cov(X_s,X_s)$
>
> 从意义理解:
>
> 协方差衡量的是两个不同随机变量之间的关系。当我们讨论同一个随机变量的协方差（即 $s = t$），这个度量变成了它**自身**随机变量的波动性，即方差

### 协方差与时间序列

#### <span id="CovExample1"> Example1 </span>

$\text{white noise } \{w_t\}$  

$\gamma(s,t)?$

- 当$s=t$ 时, 方差退化成**协方差**

$\gamma(s,s)=Cov(w_s,w_s)=Var(w_s)=\sigma ^2$

- 当$s\neq t$ 时, **由于白噪声在不同时间点中是互相独立的**
  

$\gamma(s,r)=Cov(w_s,w_r)=0$

> 或者我们利用公式推导:
>
> $Cov(w_s,w_r)=E[(w_s - \mu_s)(w_r - \mu_r)]$
>
> 由于$w_s = \mu_s$, $w_r = \mu_r$
>
> $Cov(w_s,w_r)=0$

<h4 id="Cov-Example2"> Example2 </h4>

$v_t=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1})$

$\gamma(s,t)?$

$$
\begin{align}
\gamma(s,t)&=\text{cov}(v_s,v_t)\\
&=\text{cov} \{\frac{1}{3}(w_{s-1}+_{s}+w_{s+1}),\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1})\}\\ 
\end{align}
$$

根据缩放不变形,各提取$\frac{1}{3}$

$$
\begin{align}
\gamma(s,t)
&=\text{cov} \{\frac{1}{3}(w_{s-1}+_{s}+w_{s+1}),\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1})\}\\ 
&=\frac{1}{9}\text{cov} \{(w_{s-1}+w_{s}+w_{s+1}),(w_{t-1}+w_{t}+w_{t+1})\}
\end{align}
$$


根据协方差的分配性质

$$
\gamma(s, t) = \frac{1}{9} \left( \text{Cov}(w_{s-1}, w_{t-1}) + \text{Cov}(w_{s-1}, w_t) + \text{Cov}(w_{s-1}, w_{t+1}) + \cdots \right)
$$

这是一个对所有组合 $(w_{s-1}, w_{t-1})$、$(w_s, w_t)$ 等的协方差求和的过程

由 [Example1](#CovExample1) 我们知道, **对于白噪声, 任意不同时刻的协方差都为0**, 换言之, 在所有组合中, **只有时间相等的协方差为非0, 且为$\sigma^2$**

- 当 $s=t$ 时

$$
\gamma(t, t) = \frac{1}{9} \left( \text{cov}(w_{t-1}, w_{t-1}) + \text{cov}(w_t, w_t) + \text{cov}(w_{t+1}, w_{t+1}) \right)
= \frac{3}{9} \sigma_w^2
$$

- 当$s=t+1 / s=t-1$ 时

$$
\gamma(t + 1, t) = \frac{1}{9} \left[ \text{cov}(w_t, w_t) + \text{cov}(w_{t+1}, w_{t+1}) \right]
= \frac{2}{9} \sigma_w^2
$$

- 当$s=t+2 / s=t-2$ 时

$$
\gamma(t + 2, t)
= \frac{1}{9} \text{cov}(w_{t+1}, w_{t+1})
= \frac{1}{9} \sigma_w^2
$$

综上:

$$
\gamma(s, t) = \begin{cases} 
\frac{3}{9} \sigma_w^2 & s = t, \\
\frac{2}{9} \sigma_w^2 & |s - t| = 1, \\
\frac{1}{9} \sigma_w^2 & |s - t| = 2, \\
0 & |s - t| > 2 
\end{cases}
$$

#### Example3

$v_t = \sum_{j=1}^{t} w_j$

$\gamma(s,t)?$

$v_t$ 是多个白噪声的求和, $s , t$ 代表求和的时间长度

由 [[#协方差与时间序列#Example3]] 我们知道, 要求多个白噪声对应随机变量的协方差, 只需要看在数轴上重合了多少个点

$$
\gamma(s, t) = \text{cov}(x_s, x_t) = \text{cov} \left( \sum_{j=1}^{s} w_j, \sum_{k=1}^{t} w_k \right)
= \min\{s, t\} \sigma_w^2
$$

## Lecture2

### Strict stationarity 严格平稳性

定义：当满足
$$
(x_{t_1}, \dots, x_{t_k}) \quad \text{and} \quad (x_{t_1+h}, \dots, x_{t_k+h})
$$
有相同的联合分布的时候，时间序列 ${X_t}$ 被称为严格平稳（或强平稳）

通常利用公式：
$$
\mathbb{P}\{x_{t_1} \leq c_1, \dots, x_{t_k} \leq c_k\} = \mathbb{P}\{x_{t_1+h} \leq c_1, \dots, x_{t_k+h} \leq c_k\}
$$
来表示

>   为什么 $\mathbb{P}$ 中仅仅要求“小于某个值 $c$”？
>
>   GPT:
>
>   这是因为在概率论中，事件通常是用不等式表示的。比如说，表示随机变量xt，落在某个范围之内的概率，我们通常用 $x_{t_1}≤ c1$  这样的不等式来描述。它表示“时间点 $t_1$ 处的随机变量 $X_{t_1}$，小于等于 $c_1$ 的概率“。类似地，公式中表达的是多个随机变量同时满足某些不等式的概率。
>
>   对于严格平稳性来说，关心的是这些随机变量在不同时间点是否有相同的联合分布。通过不等式 $\mathbb{P}\{x_{t_1} \leq c_1, \dots, x_{t_k} \leq c_k\}$，我们可以测量在多个时间点上这些变量同时满足条件的概率。如果这些概率在任何时间偏移$h$下都相等，那么我们就说这个时间序列是严格平稳的。

>   为什么严格平稳性是要求联合分布的概率相同，⽽不是⽅差和均值？
>
>   GPT：
>
>   严格平稳性要求联合分布的概率相同，而不仅仅是均值和方差相同，是因为它关注随机过程的完整统计特性。均值和方差只是部分特征，无法完全描述过程的行为，而联合分布包含了所有可能的统计信息，确保了过程在时间上的一致性。严格平稳性因此为分析和预测随机过程提供了一个更强大和全面的框架。

严格平稳性和相关性的关系：
$$
\begin{align}

\mathbb{P}\{x_{t_1} \leq c_1, \dots, x_{t_k} \leq c_k\} &= \mathbb{P}\{x_{t_1+h} \leq c_1, \dots, x_{t_k+h} \leq c_k\} \\
&\Downarrow\\
\mathbb{P}\{x_s \leq c_1, x_t \leq c_2\} &= \mathbb{P}\{x_{s+h} \leq c_1, x_{t+h} \leq c_2\} \\
&\Downarrow\\
\gamma(s, t) &= \gamma(s+h, t+h)
\end{align}
$$


### Weak stationarity 弱平稳性

与严格平稳性相比，弱平稳性对时间序列的约束条件较少，而不需要每个时间点的联合分布都保持不变。

定义：时间序列 $\{X_t\}$ 被称为弱平稳的，如果满足以下条件：

1.   均值独立于时间

$$
E[X_t]=μ \quad\forall t
$$

2.   协方差只依赖于时间间隔

$$
\gamma(s, t) = \gamma(s+h, t+h)
$$

### 自协方差函数 $\gamma(h)$

公式定义：
$$
\gamma(h) = \text{Cov}(X_t, X_{t+h}) = \mathbb{E}[(X_t - \mu)(X_{t+h} - \mu)]
$$
其中：

-   $h$ 是时间差（lag），表示两个时刻之间的时间间隔
-   $\mu$ 是时间序列的均值，假设时间序列是**弱平稳**的（均值不随时间变化）

###  $\gamma(h)$ 的基本性质

1.   $\gamma(0) \geq 0$
2.   $|\gamma(h)|\leq r(0)$
     协方差永远不可能与自己的协方差(方差)更大， 换言之自己与自己的相关性永远更大
3.   $\gamma(h)=\gamma(-h)$
     由于协方差性质$Cov(X,Y)=Cov(Y,X)$，可得自协方差具有对称性

### Autocorrelation Function 自相关函数

定义：

自相关函数 $\rho(h)$ 衡量的是在滞后h个时间单位时，时间序列在时刻$t$和$t +h$ 之间的相关性。其定义如下：
$$
\rho(h)=\frac{\gamma(h)}{\gamma(0)}=\frac{\text{Cov}(X_{t+h}, X_t)}{\text{Cov}(X_t, X_t)}
$$

>   为什么不是$\rho(h)=\frac{\gamma(t+h)}{\gamma(t)}$？
>
>   由于我们假定了**弱平稳性**，自协方差函数只依赖于时间滞后 $h$，而不是具体的时刻  $t$



